# Stream2Graph 数据收集执行计划

## 目标

构建 **8000条** 高质量数据集，数据来源分布：

| 来源 | 目标数量 | 占比 | 说明 |
|-----|---------|------|------|
| GitHub | 2,000-2,500条 | 25-30% | 真实代码仓库 |
| HuggingFace | 500-1,000条 | 6-12% | 公开数据集 |
| 学术论文 | 300-500条 | 4-6% | Paper2SysArch等 |
| 图表库/其他 | 200-300条 | 3-4% | Mermaid官方等 |
| 合成补充 | 4,000-5,000条 | 50-62% | 模板生成 |
| **总计** | **8,000条** | **100%** | |

## 执行步骤

### 阶段1：GitHub数据收集（4-6小时）

**需要你提供**：GitHub Personal Access Token

**收集方法**：
```bash
# 使用多源收集器
python multi_source_collector.py --github-token YOUR_TOKEN --github-target 2500

# 或单独运行GitHub收集
python github_crawler.py --token YOUR_TOKEN --type all --delay 1.5
```

**搜索策略**：
- `flowchart extension:mmd` → 流程图
- `sequenceDiagram extension:mmd` → 时序图
- `classDiagram extension:mmd` → 类图
- `graph TD extension:mmd` → Graphviz图
- `plantuml extension:puml` → PlantUML图

**预期结果**：
- 从500+个不同仓库收集
- 覆盖软件工程、系统设计、架构文档等真实场景
- 代码风格多样化

---

### 阶段2：HuggingFace数据集（1-2小时）

**自动执行**，无需额外配置：

```python
# 尝试加载的数据集
1. diagrams (如果存在)
2. code-diagrams (如果存在)
3. mermaid-charts (如果存在)
4. software-architecture (如果存在)
```

**备选方案**：
- 如果标准数据集加载失败，使用公开JSON数据源
- 或生成高质量的基于真实模板的合成数据

---

### 阶段3：其他来源（30分钟）

**自动收集**：

1. **学术论文数据**
   - Paper2SysArch风格架构图
   - DOC2CHART风格统计图
   - 从论文开源仓库获取

2. **公开图表库**
   - Mermaid官方示例
   - Draw.io模板
   - PlantUML标准库

3. **本地数据**
   - test_dataset目录中的有效文件

---

### 阶段4：数据清洗与验证（2-3小时）

**质量检查**：
```python
# 1. 编译验证
- 使用Mermaid CLI验证语法
- 过滤编译失败的代码

# 2. 内容过滤
- 去除敏感信息（密码、API Key等）
- 去除过于简单（<3节点）或过于复杂（>50节点）的图
- 去除重复/高度相似的代码

# 3. 统计验证
- 确保各类型分布符合目标
- 确保来源多样性
```

---

### 阶段5：对话生成（6-8小时，可选）

**方法A：GPT-4 API（推荐，质量高）**
```python
# 为每条代码生成自然对话
# 成本估算：4000条 × 1000 tokens × $0.01/1K = $40
```

**方法B：本地模型（成本低）**
```python
# 使用ChatGLM或类似开源模型
# 需要GPU资源
```

**方法C：模板化生成（当前做法）**
```python
# 基于图表类型使用预设对话模板
# 成本低，但多样性稍差
```

---

### 阶段6：数据整合与划分（30分钟）

```python
# 合并所有来源
# 去重
# 分层抽样划分训练/验证/测试集
# 生成最终的数据集卡片
```

---

## Token安全使用说明

### 如何申请GitHub Token

1. 登录GitHub → 右上角头像 → **Settings**
2. 左侧最下方 → **Developer settings**
3. **Personal access tokens** → **Tokens (classic)** → **Generate new token**
4. 配置：
   - **Note**: `Stream2Graph Data Collection`
   - **Expiration**: 7天（或你希望的期限）
   - **Scopes**: 只勾选 `public_repo`（只读取公开仓库）
5. 点击 **Generate token**
6. **立即复制token**（只显示一次）

### 如何安全地给我使用

**方式1：环境变量（推荐，最安全）**
```bash
# Windows
set GITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxx

# 然后运行收集
python multi_source_collector.py
```

**方式2：命令行参数**
```bash
python multi_source_collector.py --github-token ghp_xxxxxxxxxxxxxxxxxxxx
```

**方式3：配置文件**
```bash
# 创建 .env 文件
echo GITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxx > .env

# 收集器会自动读取
python multi_source_collector.py
```

### Token权限说明

- **只读公开仓库**：Token只用于搜索和下载公开代码
- **不会修改任何数据**：只有读取权限
- **可随时撤销**：在GitHub设置中可随时删除Token
- **有过期时间**：建议设置7-30天过期

---

## 成本估算

### GitHub API
- **免费**：使用Token后5000次请求/小时
- **我们的需求**：约3000-4000次请求
- **成本**：$0

### GPT-4对话生成（可选）
- **Token消耗**：~400万tokens
- **成本**：~$40-60
- **替代方案**：使用模板生成（$0）

### 其他
- **存储**：~500MB
- **计算**：本地运行，无需云服务

**总计成本**：$0（不使用GPT-4）或 $40-60（使用GPT-4）

---

## 时间安排

| 阶段 | 时长 | 是否需要你的参与 |
|-----|------|----------------|
| GitHub收集 | 4-6小时 | 需要提供Token |
| HuggingFace | 1-2小时 | 自动执行 |
| 其他来源 | 30分钟 | 自动执行 |
| 数据清洗 | 2-3小时 | 自动执行 |
| 对话生成 | 6-8小时 | 自动执行（可选API） |
| 整合划分 | 30分钟 | 自动执行 |
| **总计** | **14-20小时** | **大部分自动** |

**实际你的时间投入**：
- 提供Token：5分钟
- 等待收集完成：无需操作（可过夜运行）
- 验收结果：30分钟

---

## 预期产出

```
stream2graph_dataset/
├── multi_source/
│   ├── github/              # 2000-2500条
│   │   ├── gh_flowchart_0001.json
│   │   └── ...
│   ├── huggingface/         # 500-1000条
│   │   └── ...
│   ├── other/               # 500-800条
│   │   └── ...
│   └── collection_report.json  # 收集报告
└── 05_final/                # 整合后的最终数据集
    ├── mix_00001/           # 真实数据样本
    ├── syn_00001/           # 合成数据样本
    └── README.md
```

---

## 下一步行动

### 立即执行

**请执行以下操作之一**：

**选项A：提供Token开始收集（推荐）**
```
1. 按上述步骤申请GitHub Token
2. 通过安全方式给我：
   - 设置环境变量后告诉我"已设置"
   - 或者直接粘贴token（收集完成后立即删除）
3. 我立即开始数据收集
```

**选项B：先测试一个小样本**
```
如果你担心，我可以先用无Token模式收集少量数据（50-100条）
让你看到效果后再提供Token进行大规模收集
```

**选项C：跳过GitHub，使用其他来源**
```
如果不方便提供Token，我们可以：
- 增加HuggingFace数据比例
- 增加合成数据比例
- 从其他公开数据源收集
```

---

## 常见问题

### Q: Token会泄露吗？
**A**: 不会。
- Token只用于本次数据收集
- 不会写入任何文件（除非你在.env中保存）
- 收集完成后可以立即在GitHub上删除Token

### Q: 没有Token能收集吗？
**A**: 可以，但有严重限制。
- 无Token：60次请求/小时
- 有Token：5000次请求/小时
- 我们需要约3000-4000次请求，无Token需要50+小时

### Q: HuggingFace数据一定存在吗？
**A**: 不一定。
- 我编写的代码会尝试加载已知数据集
- 如果不存在，会自动使用备选方案
- 不会因此失败

### Q: 收集过程中断怎么办？
**A**: 支持断点续传。
- 每个下载的文件单独保存
- 中断后可以从中断处继续
- 不会重复下载已存在的文件

---

**准备好了吗？请提供GitHub Token，我立即开始！**
