{
  "id": "ot_pie_00846",
  "source": "other",
  "source_url": "https://github.com/anachary/zig-ai-platform/blob/5be2c99bfc7c013e09802f7f6fd85b0c20aa2f78/docs/diagrams/ollama_gguf_llama_flow.mmd",
  "source_type": "github_search_special",
  "github_repo": "anachary/zig-ai-platform",
  "diagram_type": "pie",
  "code": "sequenceDiagram\n    autonumber\n    participant CLI as CLI (v2-inference)\n    participant Core as core/api.generateN\n    participant Tok as Tokenizer (GGUF BPE)\n    participant RT as LLaMA Runtime (forward)\n    participant WS as WeightStore (dequant + cache)\n    participant Reg as Registries (rmsnorm/rope/attn/act)\n    participant KV as KV Cache\n    participant Samp as Sampling (greedy/top-k/top-p)\n    participant Out as Writer\n\n    CLI->>Core: prompt, flags (temperature, top-k, top-p, max-tokens, cache-mb)\n    Core->>Tok: tokenize(prompt)\n    Tok-->>Core: token ids\n\n    loop for each decode step\n        Core->>RT: forward(ctx_tokens) [measure fwd ms]\n        activate RT\n        RT->>Reg: resolve impls (rmsnorm, rope:llama, attn:cpu, act:swiglu)\n        note right of RT: for layer in [0..L)\n        RT->>WS: dequant Wq/Wk/Wv (cached?)\n        WS-->>RT: float32 weights (LRU cache)\n        RT->>Reg: rmsnorm(x)\n        RT->>RT: matmul x·Wq, x·Wk, x·Wv\n        RT->>Reg: rope.apply(q_head, k_head, theta)\n        RT->>KV: append K_t, V_t (layer, t)\n        KV-->>RT: K[0..t], V[0..t]\n        RT->>Reg: attn.compute(q, K[:t], V[:t])\n        RT->>WS: dequant Wo (cached?)\n        WS-->>RT: Wo\n        RT->>RT: x += attn_out·Wo\n        RT->>Reg: rmsnorm(x)\n        RT->>WS: dequant Wgate, Wup, Wdown (cached?)\n        WS-->>RT: Wgate/Wup/Wdown\n        RT->>Reg: act.swiglu(x·Wgate, x·Wup)\n        RT->>RT: x += (up·Wdown)\n        deactivate RT\n        RT-->>Core: hidden x\n        Core->>WS: dequant Wout (cached?)\n        WS-->>Core: Wout\n        Core->>Core: logits = x·Wout\n        Core->>Samp: select next token\n        Samp-->>Core: token id\n        Core->>Tok: detokenize(token id)\n        Tok-->>Core: utf8 piece\n        Core->>Out: stream piece [tN: Xms fwd:Yms]\n    end\n    Core-->>CLI: completed text\n\n",
  "content_size": 1817,
  "collected_at": "2026-02-06T10:47:50.697565",
  "compilation_status": "failed",
  "license": "none",
  "license_name": "No License",
  "license_url": "",
  "repo_stars": 5,
  "repo_forks": 2,
  "repo_owner": "anachary",
  "repo_name": "zig-ai-platform",
  "repo_description": "High-performance, modular AI inference platform built in Zig. Deploy models from IoT devices to distributed clusters with 10x faster inference, 50% less memory usage, and zero",
  "repo_language": "Zig",
  "repo_topics": [],
  "repo_created_at": "2025-06-27T13:10:45Z",
  "repo_updated_at": "2026-01-22T12:21:10Z",
  "compilation_error": "[WinError 2] 系统找不到指定的文件。"
}