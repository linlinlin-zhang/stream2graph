{
  "id": "ot_journey_01134",
  "source": "other",
  "source_url": "https://github.com/Chrouos/RAG-base-Automatic-Peer-Review/blob/aa1865aded18fdbbdfa055f0cf4f18537243e450/dataset/NeurIPS%202024/Accept%20(poster)-standardization_review/FMrNus3d0n.mmd",
  "source_type": "github_search",
  "github_repo": "Chrouos/RAG-base-Automatic-Peer-Review",
  "diagram_type": "journey",
  "code": "**Summary:**\nThe paper introduces GuardT2I, a novel framework designed to enhance the robustness of Text-to-Image (T2I) models against adversarial prompts. GuardT2I utilizes a large language model to conditionally transform text guidance embeddings into natural language for effective adversarial prompt detection, maintaining the performance of the T2I models. Extensive experiments demonstrate that GuardT2I outperforms commercial detectors and can detect adversarial intentions without compromising the quality of generated images. The paper is well-written, clearly explaining the motivation and methodology behind GuardT2I, and includes comprehensive evaluations across diverse scenarios.\n\n**Strengths:**\n- The paper is well-written, clear, and easy to follow, with effective use of figures to illustrate concepts.\n- GuardT2I presents a novel approach to adversarial prompt filtering in commercial-level T2I generative models, which is extensible and compatible with any other LLM architectures.\n- Comprehensive evaluations are conducted with detailed implementation setups, effectively verifying the effectiveness of the proposed method, especially in adaptive attacks.\n- The motivation and idea of the paper are novel, clear, and well-explained, addressing an important issue of defending against adversarial prompts, which is a timely and relevant topic given the increasing popularity and deployment of T2I generation models.\n- GuardT2I outperforms commercial adversarial prompt detectors in many scenarios, demonstrating practical importance through evaluations on adaptive attacks.\n\n**Weaknesses:**\n- The effectiveness of the proposed method heavily relies on the performance of the cLLM, which could be limited by the quality of its training data.\n- The introduction and related work sections note that model fine-tuning approaches compromise image quality in normal use cases, and the paper lacks a direct comparison with other types of defenses such as SafetyChecker, Safe Latent Diffusion, and concept removal methods.\n- The paper lacks evaluation and comparison on the standard text-to-image generation task, and the selection of evaluated adversarial prompts is not well motivated.\n- There is a lack of clarity in the settings of Table 6, particularly concerning the inference time of GuardT2I, which depends on the length of the recovered prompt, and the inference time of SafetyChecker, which does not depend on the input prompt.\n- The paper does not discuss a key related work published in 2023, which should be discussed in the related work section, and the authors should evaluate whether their proposed defense can effectively mitigate it in terms of performance.\n- The paper does not address the potential bias within the c.LLM, which could lead to biased interpretations of prompts, such as scenes involving cultural background.\n\n**Questions:**\n- What is the additional latency in running the proposed defenses?\n- How is the approach able to infer the actual meaning of NSFW prompts correctly during inference, given it learns to reconstruct NSFW prompts during training?\n- What is the impact of GuardT2I on benign text prompts?\n- I also want to know if the training dataset for GuardT2I includes adversarial prompts generated by methods like Sneakyp. How is the generalization of the trained LLM ensured?\n- Can the authors provide more details related to the experiment settings in Table 6, particularly concerning the inference time of GuardT2I and SafetyChecker?\n- What about the performance of GuardT2I on other T2I models like SD2.0, SD2.1, Midjourney, DALLE-3, and so on?\n\n**Soundness:**\n3 good\n\n**Presentation:**\n3 good\n\n**Contribution:**\n3 good\n\n**Rating:**\n7 accept, but needs minor improvements\n\n**Paper Decision:**\n- Decision: Accept\n- Reasons: The paper presents a novel approach to enhancing the robustness of T2I models against adversarial prompts, which is a significant contribution to the field. The methodology is well-explained, and the experiments demonstrate the effectiveness of the proposed framework across diverse scenarios. The reviewers have provided positive scores, and the Area Chair (AC) has agreed with their assessment, noting that the paper should be accepted to NeurIPS. However, the authors are encouraged to address the reviewers' comments in the camera-ready version to enhance the clarity and depth of the paper.",
  "content_size": 4376,
  "collected_at": "2026-02-06T11:05:44.615522",
  "compilation_status": "failed",
  "compilation_error": "[WinError 2] 系统找不到指定的文件。",
  "license": "other_source",
  "license_name": "Other Source (GitHub-based)"
}