{
  "id": "gl_block-beta_00345",
  "source": "gitlab",
  "source_url": "https://github.com/Chrouos/RAG-base-Automatic-Peer-Review/blob/aa1865aded18fdbbdfa055f0cf4f18537243e450/dataset/NeurIPS%202024/Accept%20(poster)-standardization_review/i8JaxY7tDI.mmd",
  "source_note": "collected_from_github_additional",
  "github_repo": "Chrouos/RAG-base-Automatic-Peer-Review",
  "diagram_type": "block-beta",
  "code": "**Summary:**\nThe paper introduces Read-ME, a novel framework that transforms pre-trained large language models (LLMs) into Mixture-of-Experts (MoE) models by decoupling the router from the MoE backbone. This allows for pre-gating and lookahead scheduling, enhancing memory management and batching during inference. The methodology involves identifying top-activated channels from a dataset to extract different experts, which are then routed using a 1-layer transformer block. The approach aims to strike a balance between efficiency and performance, with significant improvements in latency and task performance compared to existing models. The paper also explores the potential of Read-ME in resource-constrained settings, demonstrating its effectiveness in low-latency and high-throughput serving.\n\n**Strengths:**\n- The paper is well-written, well-organized, and easy-to-follow, effectively bridging the gap between algorithmic advancements and system-level optimizations.\n- The introduction of a pre-gating, shared router decoupled from the MoE backbone is a significant innovation, allowing for pre-computation and lookahead scheduling, addressing inefficiencies in traditional layer-wise routing systems.\n- The experimental results are comprehensive and demonstrate significant improvements in mean and tail latency, as well as MMLU performance and latency reductions.\n- The paper provides a thorough comparison with various baselines, including both dense and MoE models, demonstrating the superiority of Read-ME across different metrics.\n\n**Weaknesses:**\n- The experimental subject is limited, primarily focusing on the LLaMA-2 model, which raises concerns about the scalability and generalization of the proposed method to other model types and larger scales.\n- The motivation behind pruning LLMs into smaller MoEs is not clearly justified, especially when compared to pruning an LLM to create a smaller but dense model with similar or fewer activated parameters.\n- The paper could benefit from a more detailed explanation of the pre-gating and batching algorithms, potentially with pseudocode or flow diagrams to aid reproducibility.\n- The main results of Table 1 do not support that the proposed method is at a better accuracy vs inference-cost trade-off, and the overall accuracy cost of pruning and ensemble is substantial.\n- The observation in Figure 2 and the assumption in Section 2.3 regarding the use of only a few routing paths during inference are not convincingly supported.\n\n**Questions:**\n- If the top activated neurons are kept, should equation 3 be an argmax?\n- What is the impact of the routing distillation loss? Can you provide an ablation experiment where it is not used to access its impact?\n- How would Figure 3 look with READ-ME instead of standard MoEs? This would be a good visualization to compare the methods.\n- How were the baseline numbers in Figure 4 obtained? Did the authors rerun the baselines? Can you confirm that all these baselines start from the same base model?\n- The \"cost\" column of table 1 is misleading; given that your model starts from llama-2, you must either include the compute to create llama-2 in your analysis or only monitor the additional compute starting from llama 2.\n- Could you report additional experiments on Gemma or Mistral to address the limited evaluation to Llama2 only?\n\n**Soundness:**\n3 good\n\n**Presentation:**\n3 good\n\n**Contribution:**\n3 good\n\n**Rating:**\n6 marginally above the acceptance threshold\n\n**Paper Decision:**\n- Decision: Accept\n- Reasons: The paper presents a novel approach to enhancing the efficiency of pre-trained LLMs by transforming them into post-hoc MoE models, which is technically solid and has a high impact on at least one sub-area. The methodology, while not flawless, is innovative and addresses significant bottlenecks in deploying MoE models. The experimental results are thorough and demonstrate significant improvements in latency and task performance compared to existing models. The decision to accept is based on the consensus from the reviewers, who praised the submission for its motivation, presentation, and experimental validation. However, the paper does need to address the comments and questions raised during the review process in the final version.",
  "content_size": 4267,
  "collected_at": "2026-02-06T14:07:55.567398",
  "compilation_status": "failed",
  "compilation_error": "[WinError 2] 系统找不到指定的文件。",
  "license": "gitlab_repo",
  "license_name": "GitLab Repository",
  "license_url": "https://gitlab.com"
}