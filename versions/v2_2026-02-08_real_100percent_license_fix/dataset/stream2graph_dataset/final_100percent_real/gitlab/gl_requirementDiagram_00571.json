{
  "id": "gl_requirementDiagram_00571",
  "source": "gitlab",
  "source_url": "https://github.com/Chrouos/RAG-base-Automatic-Peer-Review/blob/aa1865aded18fdbbdfa055f0cf4f18537243e450/dataset/NeurIPS%202024/Accept%20(poster)-standardization_review/UTNZKl5BUc.mmd",
  "source_note": "collected_from_github_additional",
  "github_repo": "Chrouos/RAG-base-Automatic-Peer-Review",
  "diagram_type": "requirementDiagram",
  "code": "**Summary:**\nThe paper introduces a novel approach to gradual domain adaptation using distributionally robust optimization (DRO), focusing on controlling the error propagation and improving generalization across domains. The method involves iteratively performing manifold-constrained Wasserstein DRO and pseudo-labeling on the sequence of domains, with theoretical guarantees provided for model adaptation across successive datasets. The approach is validated through experiments and theoretical analysis, demonstrating the ability to eliminate error propagation in specific settings. The paper also discusses the implications of the assumptions required for the theoretical results to hold and provides a non-expansive error bound for an increasing number of domains.\n\n**Strengths:**\n- The paper is well-written, structured, and effectively communicates complex theoretical concepts in a clear and organized manner.\n- The motivation of employing DRO as a theoretical framework to address gradual domain adaptation is clear and reasonable.\n- The paper makes significant theoretical contributions, providing rigorous guarantees on model adaptation and generalization errors across domains.\n- The algorithm provides a bounded error regardless of T, and for appropriately constrained distributions, the error can be demonstrated to be linear or even entirely eradicated.\n- The paper extends theoretical results to a more general class of distributions, referred to as \"expandable\" distributions, with learnable classifiers.\n- The theoretical contributions are substantial, providing rigorous guarantees on model adaptation and generalization errors across domains.\n- The paper is structured to maintain readability, making it accessible to readers.\n\n**Weaknesses:**\n- The experimental study is very limited and hidden in Appendix D.\n- The relationship to existing work, both classical theory of domain adaptation and on gradual domain adaptation, could be clarified.\n- The proof of the theoretical results requires overly stringent conditions, such as the assumption of expandable distributions, the use of smooth mappings, and the requirement of distributions characterized by favorable properties.\n- The clarity of the paper could be improved, particularly in discussing the advantages or improvements over related theory for GDA and in justifying the use of the DRO algorithm.\n- The paper primarily focuses on the error within the domain PT, but the error of θ* in the previous domains P1, …, PT−1 remains uncertain.\n- The theoretical results rely on relatively toy data distributions and stringent properties, which could lead to limited practical applicability.\n- There are many typos and the readability is fair.\n\n**Questions:**\n- Could the authors discuss possible extensions to non-binary settings and settings where the domain shift is applied to the joint distribution (Z) rather than the marginal (X)?\n- Is definition 4.1 correctly presented? The left hand side of the inequality is independent of the Borel set A, while the right hand side is dependent on A.\n- How to effectively compute the following WDRO problem under the manifold constraint G?\n- What are the challenges in extending theoretical results to multi-class data distributions?\n- The bond in Theorem 2.3 shows that the target error can be dominated by the source risk with the factor gλ(⋅)∘T. Can the factor gλ(⋅)∘T be monotonically reduced by the increase of T?\n- The minimum form in Theorem 3.1 is not tight for small distances between adjacent domains. Can the minimum form be improved?\n- The proof of Theorem 3.1 states that one can only upper bound ∫ℙ[ℓ(x, y)] and 1−∫ℙ[ℓ(x, y)]. Should the conclusion of Theorem 3.1 be revised accordingly?\n\n**Soundness:**\n3 good\n\n**Presentation:**\n3 good\n\n**Contribution:**\n3 good\n\n**Rating:**\n7 accept, but needs minor improvements\n\n**Paper Decision:**\n- Decision: Accept\n- Reasons: The paper presents a novel approach to gradual domain adaptation using DRO, providing theoretical guarantees for model adaptation across successive datasets. The approach is supported by theoretical analysis that demonstrates the control of error propagation and improvement of generalization across domains. The reviewers have provided positive feedback on the paper, highlighting its contribution as the first to establish a non-expansive error bound for an increasing number of domains. Despite some concerns regarding the experimental study and the need for proofreading, the overall assessment is positive, and the paper is recommended for acceptance with minor improvements.",
  "content_size": 4578,
  "collected_at": "2026-02-06T14:12:32.760767",
  "compilation_status": "failed",
  "compilation_error": "[WinError 2] 系统找不到指定的文件。",
  "license": "gitlab_repo",
  "license_name": "GitLab Repository",
  "license_url": "https://gitlab.com"
}