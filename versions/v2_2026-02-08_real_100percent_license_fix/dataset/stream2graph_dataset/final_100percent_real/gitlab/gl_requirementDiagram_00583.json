{
  "id": "gl_requirementDiagram_00583",
  "source": "gitlab",
  "source_url": "https://github.com/Chrouos/RAG-base-Automatic-Peer-Review/blob/aa1865aded18fdbbdfa055f0cf4f18537243e450/dataset/NeurIPS%202024/Accept%20(poster)-standardization_review/0VeSCjRDBy.mmd",
  "source_note": "collected_from_github_additional",
  "github_repo": "Chrouos/RAG-base-Automatic-Peer-Review",
  "diagram_type": "requirementDiagram",
  "code": "**Summary:**\nThe paper introduces a novel reinforcement learning (RL) framework for auto-regressive text generation, focusing on knowledge distillation of large language models (LLMs). It frames the knowledge distillation problem as a task of minimizing the imitation gap between teacher and student policies, using an adversarial moment-matching approach. The method involves an adversarial training algorithm that optimizes the imitation gap by estimating and minimizing on-policy and off-policy moment-matching distances. The approach is evaluated on instruction-following and task-specific datasets, showing improved performance over traditional distribution-matching methods. The paper also discusses the computational costs and memory requirements of the proposed method, acknowledging its potential limitations.\n\n**Strengths:**\n- The paper introduces a novel approach to knowledge distillation for LLMs by reformulating it as a moment matching problem, which is a novel perspective.\n- The method is well-structured, clearly presented, and facilitates understanding, making it accessible to readers.\n- Theoretical analysis demonstrates that the momentum-matching target provides a tighter bound for minimizing the imitation gap compared to conventional distribution-matching targets.\n- Comprehensive experiments empirically demonstrate that the proposed method outperforms existing distribution-matching methods in performance.\n- The paper is well-written, clear, and presents a clear, enjoyable narrative, which facilitates ease of understanding for the reader.\n\n**Weaknesses:**\n- The paper lacks a detailed comparison of the training costs of each of the compared methods, particularly in terms of computational and memory resources.\n- There is a lack of discussion on the computational complexity or additional overhead in memory of having multiple models and requiring rollouts from the teacher and student model while training.\n- The method requires an auxiliary network for Q-value estimation, which adds complexity to the training system.\n- The paper does not provide details on the training of auxiliary models used to compute Q-value functions, nor does it discuss the impact of these models on the effectiveness of the method.\n- The generalizability of the method across models with different architectures is not demonstrated, which leaves the robustness of the approach across diverse settings untested.\n\n**Questions:**\n- How exactly is the Q-value function parameterized? Is it an extra head on the model, or a new model entirely?\n- Did you use a baseline to reduce variance when using policy gradients for training the student function? What is the variance of these policy gradients?\n- Have you considered lightweight baseline methods, such as those presented in [1], to potentially reduce the computational and memory costs?\n- Can you provide an ablation of the different elements of the approach, such as investigating the relative importance of the two upper bounds?\n- Could you provide a detailed comparison of resource consumption and memory costs relative to the baseline methods?\n- Is it possible to directly fine-tune teacher models to compute Q-value functions?\n\n**Soundness:**\n3 good\n\n**Presentation:**\n3 good\n\n**Contribution:**\n3 good\n\n**Rating:**\n6 marginally above the acceptance threshold\n\n**Paper Decision:**\n- Decision: Accept\n- Reasons: The paper presents a novel approach to knowledge distillation for LLMs, which is well-received by the reviewers for its novelty, methodological soundness, and empirical validation. The method's theoretical underpinnings, particularly the use of an RL framework to minimize the imitation gap, are highlighted as strengths. The experimental results, while showing some limitations in terms of computational resources and generalizability, are robust enough to establish the efficacy of the method. The reviewers have noted that the paper is well-written, clearly presented, and provides a clear narrative, which enhances its readability and accessibility. Despite some concerns regarding the clarity of parameterization and the lack of quantitative analysis, these issues have been adequately addressed in the rebuttal phase. Overall, the decision to accept is based on the paper's originality, methodological soundness, and the significant impact it could have on the field of LLM knowledge distillation.",
  "content_size": 4378,
  "collected_at": "2026-02-06T14:12:50.039680",
  "compilation_status": "failed",
  "compilation_error": "[WinError 2] 系统找不到指定的文件。",
  "license": "gitlab_repo",
  "license_name": "GitLab Repository",
  "license_url": "https://gitlab.com"
}