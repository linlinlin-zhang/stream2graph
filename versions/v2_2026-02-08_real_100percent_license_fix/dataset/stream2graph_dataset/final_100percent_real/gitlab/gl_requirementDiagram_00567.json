{
  "id": "gl_requirementDiagram_00567",
  "source": "gitlab",
  "source_url": "https://github.com/Chrouos/RAG-base-Automatic-Peer-Review/blob/aa1865aded18fdbbdfa055f0cf4f18537243e450/dataset/SEA_ICLR_2024/GPT4_consolidated_reviews_2k/1ii8idH4tH.mmd",
  "source_note": "collected_from_github_additional",
  "github_repo": "Chrouos/RAG-base-Automatic-Peer-Review",
  "diagram_type": "requirementDiagram",
  "code": "**Summary:**\nThe paper addresses the challenge of nonconvex federated learning in the presence of Byzantine workers. It introduces a Momentum Screening (MS) algorithm that is both simple and adaptable to varying levels of Byzantine worker fractions. The central claim of the paper is that MS can achieve an optimization error of O(δ²ζmax²), which is superior under conditions where ζmax is not significantly larger than ζmean. Experimentally, the paper supports its claims with results from studies using datasets like MNIST and CIFAR10, demonstrating that MS algorithm outperforms existing methods such as Centered Clipping, especially when the fraction of Byzantine workers, δ, is small. Additionally, the paper establishes a minimax optimal lower bound for this setting, contributing to theoretical understanding in federated learning under Byzantine conditions.\n\n**Strengths:**\n- The MS algorithm is appreciated for its simple and adaptable algorithmic structure, especially in settings with Byzantine fractions, making it practically appealing.\n- The paper presents a novel contribution through the minimax optimality results under \\(\\zeta_{\\text{max}}\\)-uniform gradient heterogeneity, which is significant as it is possibly the first of its kind.\n- The implementation of the MS algorithm does not require prior knowledge of the Byzantine fraction, enhancing its practical usability.\n- The performance of the MS algorithm is substantiated with numerical experiments, demonstrating superior performance especially under conditions where the Byzantine fraction is small.\n- The algorithm is noted for its computational efficiency even when adapted for better performance, leading to only a constant factor of degradation in performance as compared to the original algorithm.\n\n**Weaknesses:**\n- The paper insufficiently discusses existing literature, particularly the similarities in the rates achieved by the MS algorithm and those found in the CCLIP method. This oversight misses an opportunity to sufficiently distinguish the proposed work from existing methods.\n- There is a critical error in the paper with the requirement that \\(\\delta \\leq (\\zeta_{\\text{mean}}/\\zeta_{\\text{max}})^2\\), where various parts of the paper indicate conflicting derivations, thus casting doubts on the correctness of the proofs presented.\n- The analysis provided in the paper seems a natural extension of previous works without significant technical novelty, other than the algorithm's simplicity and its effectiveness under specific constraints.\n- Systematic typographical errors and unclear explanation sections make the paper less accessible and harder to follow.\n- The utility of momentum in the algorithm is questioned, particularly since the convergence rates provided resemble those typical of non-momentum-based methods, which raises concerns about the implementation of momentum and whether it could be optimized or omitted without impacting performance.\n\n**Questions:**\n- Could the authors clarify the practical implications of the relationship between \\(\\zeta_{\\text{max}}\\) and \\(\\zeta_{\\text{mean}}\\), particularly in real datasets used in experiments?\n- Given the considerable overlap in results with the CCLIP method discussed in the literature, can the authors detail any distinguishing benefits of the MS algorithm over CCLIP not already covered by existing methods?\n- Will the convergence rate of the MS algorithm remain unchanged if the momentum component is removed? Or could there be a more effective momentum-based method that would yield better convergence rates?\n- Can the authors suggest practical guidance or empirical methods to choose the hyperparameter \\(\\tau_t\\), particularly when \\(\\zeta_{\\text{max}}\\) is unknown a priori? This would aid in real-world applications where prior knowledge of gradient heterogeneity might not be available.\n\n**Soundness:**\n3 good\n\n**Presentation:**\n2 fair \n\n**Contribution:**\n3 good\n\n**Rating:**\n5 marginally below the acceptance threshold\n\n**Paper Decision:**\n- Decision: Accept\n- Reasons: The decision to accept the paper, recommended as a poster presentation, is based on its methodological soundness and the presentation of significant results, particularly in the context of Byzantine robust federated learning. Despite some concerns about the convergence rate, and similarities with existing methods like the CCLIP, the innovative approach of the MS algorithm, along with its theoretical merits such as the minimax optimality and proof of lower bounds, provides valuable contributions to the field. Moreover, the algorithm is practical for real-world application scenarios as evidenced by the experiments. The decision is also supported by the area chair's override of more critical reviews, noting that the authors adequately responded to concerns raised during the review process.",
  "content_size": 4838,
  "collected_at": "2026-02-06T14:12:26.500179",
  "compilation_status": "failed",
  "compilation_error": "[WinError 2] 系统找不到指定的文件。",
  "license": "gitlab_repo",
  "license_name": "GitLab Repository",
  "license_url": "https://gitlab.com"
}