{
  "id": "gl_requirementDiagram_00579",
  "source": "gitlab",
  "source_url": "https://github.com/Chrouos/RAG-base-Automatic-Peer-Review/blob/aa1865aded18fdbbdfa055f0cf4f18537243e450/dataset/SEA_ICLR_2024/GPT4_consolidated_reviews_2k/SXTmAdGjlg.mmd",
  "source_note": "collected_from_github_additional",
  "github_repo": "Chrouos/RAG-base-Automatic-Peer-Review",
  "diagram_type": "requirementDiagram",
  "code": "**Summary:**\nThe paper introduces an adaptive optimization algorithm for bilevel optimization problems, focusing on scenarios with a strongly convex inner function and a potentially non-convex outer function. Utilizing mirror descent, the proposed method achieves convergence rates of $\\mathcal{O}(1/T)$ without requiring knowledge of Lipschitz constants. Varied experimental results validate the practical merits of this method compared to approximation approaches. The work also evaluates adaptivity by assuming known strong convexity constants but not smoothness constants. The algorithm adapts step-sizes using a gradient norm accumulation and mirror descent strategy, offering adaptivity benefits with potential limitations when extending to stochastic scenarios. Concerns include possible misleading claims about its novelty, dependency on unspoken parameters like strong convexity constants, and questions regarding theoretical assumptions in non-convex settings.\n\n**Strengths:**\n- The paper proposes an innovative adaptive optimization algorithm based on mirror descent for solving bilevel optimization problems possibly involving nonconvex smooth issues with a strongly convex lower level, with less reliance on the knowledge of smoothness parameters, which enhances practical application and theoretical interest.\n- The paper is well structured, featuring a clear narrative and presentation style, making the algorithm's concepts and proof sketches easy to follow, thus aiding in understanding and replication.\n- The proposed algorithm promises adaptivity to smoothness constants and demonstrates this through numerical experiments, comparing favorably against several baselines, although it mainly considers deterministic settings.\n\n**Weaknesses:**\n- There is a claim of novelty regarding the adaptive nature of the algorithm for bilevel optimization problems, which is questionable as prior studies have tackled similar adaptive approaches in both deterministic and stochastic bilevel optimization settings.\n- The algorithm depends critically on parameters such as the strong convexity parameters of the functions involved, which contradicts its claim of full adaptivity. This dependency potentially complicates its application in cases where these parameters are not readily available or need to be estimated.\n- There's an excessive dependence on the Hessian matrix and its inversion, which might not be practical or efficient for large-scale problems. The reliance on double-loop algorithms where inner loop iterations increase with outer loop iterations further complicates its practical application.\n- Although demonstrating numerical robustness, the convergence rate achieved ($O(1/T)$) does not meet the state-of-the-art rates for convex-strongly-convex cases, and improvements on this front are suggested.\n- There's a lack of stochastic consideration where theory mainly covers deterministic cases, but practical applications often involve stochastic elements.\n- The experimental results are only plotted in terms of iterations without corresponding time measures, making it difficult to gauge real-world performance accurately.\n\n**Questions:**\n1. Given the algorithm's dependency on strong convexity parameters and its notable computational requirements (like Hessian matrix inversion), how do the authors envision practical application in large-scale and possibly less-controlled real-world scenarios?\n2. How does the proposed algorithm perform in stochastic settings, particularly when parameters such as gradient norms are unbounded? Are there planned adjustments or extensions to the adaptive designs to handle stochastic variations?\n3. Can the authors clarify the seeming contradictions and gaps in theoretical proofs, like the necessary assumptions for Theorem 2 and the conditions of the function set $\\mathcal{X}$ in various results?\n4. There appear to be missing comparisons and discussions on similar adaptive methods from recent works. Might the authors detail how their approach distinctly advances beyond these already existing methods?\n5. Could the authors provide more detailed examples and scenarios of practical application, especially explaining the choice and impact of the regularization function $h(\\cdot)$ in the proposed algorithm?\n6. In future revisions or responses, could the authors address the various editorial and grammatical issues noted across the paper to enhance clarity and professionalism in presentation?\n\n**Soundness:**\n2 fair\n\n**Presentation:**\n2 fair\n\n**Contribution:**\n2 fair\n\n**Rating:**\n3 reject, not good enough\n\n**Paper Decision:**\n- Decision: Reject\n- Reasons: The reviews collectively indicate that while the paper presents an algorithm with some practical merits, there are significant issues that undermine its acceptance. Primarily, the method's novelty and theoretical contributions are questioned due to similar existing approaches and dependencies on specific problem constants that are assumed to be known. Additionally, the paper lacks a response to reviewer concerns, as highlighted in the metareview. Further, there are potential misstatements regarding being the first of its kind and an apparent limitation in handling only deterministic cases without extending to stochastic scenarios. Consequently, the integrity and robustness of the findings are insufficient to warrant acceptance at this time.",
  "content_size": 5377,
  "collected_at": "2026-02-06T14:12:45.537582",
  "compilation_status": "failed",
  "compilation_error": "[WinError 2] 系统找不到指定的文件。",
  "license": "gitlab_repo",
  "license_name": "GitLab Repository",
  "license_url": "https://gitlab.com"
}