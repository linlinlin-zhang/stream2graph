{
  "id": "gl_packet-beta_00468",
  "source": "gitlab",
  "source_url": "https://github.com/Chrouos/RAG-base-Automatic-Peer-Review/blob/aa1865aded18fdbbdfa055f0cf4f18537243e450/dataset/SEA_ICLR_2024/GPT4_consolidated_reviews_2k/4jBL79L5QS.mmd",
  "source_note": "collected_from_github_additional",
  "github_repo": "Chrouos/RAG-base-Automatic-Peer-Review",
  "diagram_type": "packet-beta",
  "code": "**Summary:**\nThe paper introduces eleganTE, a benchmark framework for training and evaluating reinforcement learning (RL) based routing algorithms using the ns-3 network simulator. It frames traffic engineering as a Swarm Markov Decision Process (SwarMDP), allowing RL agents to adapt to new network topologies, and evaluates common routing algorithms as baselines. Despite displaying potential advances in traffic engineering through RL, the paper struggles to clearly demonstrate the efficacy of RL methods over standard shortest-path algorithms across the small network scenarios tested. There is also a lack of clarity in the experimental setup and scalability concerns, along with a need for expanded evaluations across larger and real-world network configurations.\n\n**Strengths:**\n- The SwarMDP formulation provides a way to train policies that generalize across different network topologies, an essential feature for effective routing and network management.\n- EleganTE, introduced in the paper, leverages a configurable network simulator to facilitate repeatable experiments, improving the experimental validity and reproducibility.\n- The framework uses state-of-the-art network simulators allowing for more accurate and faithful simulations which can account for various network dynamics like interference and protocol interactions.\n- The ability to generate a variety of random network topologies and use realistic traffic models (gravity model with small perturbations) makes the framework robust and versatile.\n- Various ablation studies are included, providing insights into the impact of different policy architectures, reward functions, and actor designs on the system's performance.\n- The framework aims to improve operational efficiency and reduce costs by automating network management, with potential applications beyond telecommunication networks, such as in transport networks and power grids.\n\n**Weaknesses:**\n- The SwarMDP lacks detailed information on handling heterogeneous action spaces and effective scaling to large network sizes.\n- EleganTE needs more specifics regarding implementation, particularly how network state information is translated into the monitoring graph.\n- The experimental section, primarily focusing on small networks (up to 50 nodes), does not adequately test the system's scalability and performance on larger network sizes (500+ nodes).\n- Instances of high variance in performance in some experiments suggest potential stability issues with the GNN agent.\n- Comparisons made in the paper predominantly against simplistic baseline methods (OSPF and EIGRP) do not sufficiently demonstrate the superiority or necessity of the proposed RL-based methods against more advanced state-of-the-art techniques.\n- The framework does not support scenarios with dynamically changing topologies or decentralized training and execution, which are critical for real-world distributed network environments.\n- No evaluation of the system's effectiveness on real-world network topologies, limiting the understanding of its practical applicability and performance.\n\n**Questions:**\n- Can the SwarMDP handle dynamic changes in network topology within an episode, and how does it manage such changes?\n- What specific neural network architectures are used, and how do they influence the system's performance and stability?\n- Are there instances where RL-based techniques clearly outperform standard protocols in all reported metrics, and what does this imply about the framework's effectiveness?\n- Given that SwarmMDP may assume homogenous agent collaboration, is this assumption valid in cases where network nodes behave asymmetrically depending on their position in the network graph?\n- Would a multi-objective RL approach be more appropriate for this problem considering the various metrics like packet delay and drop ratio that need to be optimized simultaneously?\n- What are the specific contributions of the proposed open-source RL-based TE framework compared to existing TE solutions, and how do the custom ns-3 modules enhance the TE process?\n- How does the proposed Markov Decision Process formulation for TE compare with traditional network TE formulations, and what advantages does it provide?\n\n**Soundness:**\n2 fair\n\n**Presentation:**\n2 fair\n\n**Contribution:**\n2 fair\n\n**Rating:**\n3 reject, not good enough\n\n**Paper Decision:**\n- Decision: Reject\n- Reasons: The primary contributions include the introduction of an evaluation framework for learning-based routing algorithms, particularly in the context of computer networks. However, significant concerns arise from the inability of RL-based routing optimization to consistently outperform simpler shortest-path baselines. The experiments reported fail to clearly justify the RL approach over these baselines, especially within small network scenarios. Additionally, the clarity in presentation, particularly in the formulation and details of the experimental setup, further detracts from the paper's strengths. The framework's scalability to larger networks and its utility in yielding meaningful conclusions remain unclear. These aggregated weaknesses, highlighted both in individual reviews and the metareview, underscore the necessity for substantial future improvements, leading to the conclusion to reject the paper.",
  "content_size": 5310,
  "collected_at": "2026-02-06T14:10:19.605036",
  "compilation_status": "failed",
  "compilation_error": "[WinError 2] 系统找不到指定的文件。",
  "license": "gitlab_repo",
  "license_name": "GitLab Repository",
  "license_url": "https://gitlab.com"
}