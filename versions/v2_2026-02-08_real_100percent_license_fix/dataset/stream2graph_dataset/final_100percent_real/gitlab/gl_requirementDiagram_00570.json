{
  "id": "gl_requirementDiagram_00570",
  "source": "gitlab",
  "source_url": "https://github.com/Chrouos/RAG-base-Automatic-Peer-Review/blob/aa1865aded18fdbbdfa055f0cf4f18537243e450/dataset/ICLR%202024/Accept%20(poster)-standardization_review/HFtrXBfNru.mmd",
  "source_note": "collected_from_github_additional",
  "github_repo": "Chrouos/RAG-base-Automatic-Peer-Review",
  "diagram_type": "requirementDiagram",
  "code": "**Summary:**\nThe paper investigates the representation distortion in Graph Neural Networks (GNNs) during the evolution of graphs over time. It introduces SMART, a method that employs self-supervised contrastive graph reconstruction to update the feature extractor, minimizing information loss during the dynamic evolution process. The authors provide theoretical proofs and numerical experiments to demonstrate the distortion of representation is inevitable and propose a way to estimate generalization loss. The study also derives a closed-form expression of the generalization error bound on synthetic data and verifies the effectiveness of the proposed method.\n\n**Strengths:**\n- The paper addresses an important and novel problem in the field of dynamic graph learning, focusing on the generalization gap prediction for temporal node classification, which has not been well-studied in the graph representation learning literature.\n- The authors provide theoretical proofs and conduct experiments across a variety of datasets, including both synthetic and real-world datasets, to demonstrate the effectiveness of the proposed method.\n- The paper is well-written, clearly presenting the problem definition, theoretical justification, and methodology, including several architecture and loss considerations.\n- The empirical results show that the proposed method, SMART, outperforms a linear regression model using the same GCN/GAT/GraphSage backbone structures, indicating the quality and robustness of SMART.\n- The ablation study is well-conducted, considering various loss configurations and hyper-parameter configurations, which helps in understanding the impact of different components of the method.\n\n**Weaknesses:**\n- The theoretical analysis is limited to single-layer GCN models, and it would be beneficial to extend this analysis to more complex models.\n- The baseline only includes simple linear regression models, which may not adequately demonstrate the advantages of the proposed method. More robust baselines, such as models that continuously acquire new node labels and get retrained on the new data, should be considered.\n- The paper lacks a related work section, which could provide a better context for the reader by discussing previous work that shows the performance degradation of GNNs over time due to representation distortion.\n- There is an inconsistency in the use of information loss in the theoretical analysis and experimental settings, which could be clarified or corrected.\n- The data augmentation in graph is considered too naive, as it only involves randomly adding or dropping edges, which may not be effective for graphs with changing edge labels.\n- The assumptions made for theoretical analysis and bound derivation are potentially too strong, and the zero-mean requirement for feature preprocessing might not be applicable in all scenarios.\n\n**Questions:**\n- Can the authors clarify the inconsistency in the use of information loss in the theoretical analysis and experimental settings?\n- How does the proposed method perform when compared to other state-of-the-art generalization error predictors, and what are the specific advantages of SMART over these methods?\n- Is Theorem 1 adjustable to multi-layer GNNs and different GNN backbone structures such as GCN/GAT/GraphSage?\n- Could the authors provide more details on the y-axis prediction loss in Figure 3 and consider using other evaluation metrics to showcase the GNN prediction performance deterioration?\n- Why was RNN chosen over other time-series models to capture the temporal variation, and how does this choice affect the overall performance of the model?\n- How will the proposed algorithm be applied to more complex real-time graphs, especially those with changing node and edge labels?\n\n**Soundness:**\n2 fair\n\n**Presentation:**\n3 good\n\n**Contribution:**\n3 good\n\n**Rating:**\n6 marginally above the acceptance threshold\n\n**Paper Decision:**\n- Decision: Accept\n- Reasons: The paper presents a novel approach to addressing the representation distortion in GNNs during graph evolution, which is a significant challenge in dynamic graph learning. The theoretical proofs and numerical experiments provide a strong foundation for the proposed method, SMART, which shows promise in improving generalization estimation in temporal node classification tasks. The methodology is well-articulated, and the empirical results demonstrate the effectiveness of SMART over linear regression models. However, the paper could benefit from a more robust baseline comparison and a more detailed discussion on the theoretical assumptions and their implications. Despite these limitations, the paper's contributions are substantial, and the methodology, if further refined, could have a significant impact on the field.",
  "content_size": 4794,
  "collected_at": "2026-02-06T14:12:30.058450",
  "compilation_status": "failed",
  "compilation_error": "[WinError 2] 系统找不到指定的文件。",
  "license": "gitlab_repo",
  "license_name": "GitLab Repository",
  "license_url": "https://gitlab.com"
}