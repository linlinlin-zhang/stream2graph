{
  "id": "gl_block-beta_00313",
  "source": "gitlab",
  "source_url": "https://github.com/Chrouos/RAG-base-Automatic-Peer-Review/blob/aa1865aded18fdbbdfa055f0cf4f18537243e450/dataset/ICLR%202024/Reject-standardization_review/I8pdQLfR77.mmd",
  "source_note": "collected_from_github_additional",
  "github_repo": "Chrouos/RAG-base-Automatic-Peer-Review",
  "diagram_type": "block-beta",
  "code": "**Summary:**\nThe paper introduces a novel activation function, AGeLU, aimed at enhancing the non-linearity in MLP modules within vision transformers. This function is integrated with concatenation operators and depthwise convolutions to potentially improve model performance. The authors provide theoretical justifications and experimental results on ImageNet-1K to support their claims. The proposed IMLP module, which includes AGeLU, is designed to reduce the number of hidden dimensions while maintaining accuracy, as demonstrated through various experiments across different vision transformer architectures.\n\n**Strengths:**\n- The paper provides a solid theoretical analysis by delving into the math of the MLP module, successfully establishing it as a non-linearity generator.\n- The introduction of AgeLU to form a more nonlinear function and the extension of non-linearity enhancement to the spatial dimension are well-explained and supported by empirical validation across various tasks and diverse transformer models.\n- The writing in the paper is well-structured, making it easy to understand, and the ideas are presented in a thoughtful and engaging manner.\n- The effectiveness of the proposed method is validated across various tasks and diverse transformer models, encompassing both isotropic and stage-wise variants.\n- The paper provides a theoretical analysis of the bounds of the modified IMLP module, providing valuable insights for parameter selection within the module.\n\n**Weaknesses:**\n- The paper primarily addresses the activation functions, but many related works are missing, which have emerged after GeLU.\n- The performance enhancements provided by the proposed activation function are marginal and non-existent in some instances.\n- The effectiveness of the proposed method heavily relies on employing depthwise convolution, which has been already recognized in many prior hybrid architectures.\n- The paper lacks a comprehensive ablation study to provide a richer understanding of the behavior and performance of the functions introduced.\n- There is ambiguity regarding whether the kernel size is consistently set to 5 for large-scale models such as DeiT-B or Swin-B, and the remarkable performance drops attributed to the addition of GeLU with four times the number of channels need further clarification.\n- The paper lacks actual latencies and throughput comparisons, which are crucial for real applications.\n\n**Questions:**\n- Can you offer the actual latencies for the proposed models?\n- Why is the additional shortcut needed for the dwconv and BN to follow it subsequently?\n- How does knowledge distillation (KD) work when training with the proposed activation function?\n- What would be the outcome if the division in Equation (5) was into more parts, say four?\n- Could you provide insights or intuitions into why such an outcome occurred in Table 3?\n- Why is there a performance decline for Swin-B and Poolformer-M48 when using the proposed activation function?\n- Introducing spatial enhancement after the GeLU operation (as 'a' in Figure 4) would help to conclusively demonstrate the impact of enhanced non-linearity in parameter reduction. Can you provide more details on this?\n\n**Soundness:**\n3 good\n\n**Presentation:**\n3 good\n\n**Contribution:**\n3 good\n\n**Rating:**\n3 reject, not good enough\n\n**Paper Decision:**\n- Decision: Reject\n- Reasons: The paper, while introducing a novel activation function and proposing a modified MLP module, suffers from significant limitations. The primary concerns include the marginal performance enhancements and the reliance on depthwise convolution, which has been previously recognized. The theoretical justifications provided do not sufficiently address the necessity of combining AGeLU with concatenation, and the experimental results do not convincingly demonstrate the effectiveness of the proposed method across a broad range of models. The paper also lacks comprehensive comparisons with other non-linear blocks and fails to provide actual latencies, which are crucial for real-world applications. These factors collectively lead to the decision to reject the paper.",
  "content_size": 4139,
  "collected_at": "2026-02-06T14:06:58.464132",
  "compilation_status": "failed",
  "compilation_error": "[WinError 2] 系统找不到指定的文件。",
  "license": "gitlab_repo",
  "license_name": "GitLab Repository",
  "license_url": "https://gitlab.com"
}