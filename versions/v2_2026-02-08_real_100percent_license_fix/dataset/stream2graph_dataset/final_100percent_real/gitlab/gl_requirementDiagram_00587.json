{
  "id": "gl_requirementDiagram_00587",
  "source": "gitlab",
  "source_url": "https://github.com/Chrouos/RAG-base-Automatic-Peer-Review/blob/aa1865aded18fdbbdfa055f0cf4f18537243e450/dataset/SEA_ICLR_2024/GPT4_consolidated_reviews_2k/WM5G2NWSYC.mmd",
  "source_note": "collected_from_github_additional",
  "github_repo": "Chrouos/RAG-base-Automatic-Peer-Review",
  "diagram_type": "requirementDiagram",
  "code": "**Summary:**\nThe paper explores how to adapt pre-trained models to new tasks without sacrificing zero-shot learning capabilities, utilizing a novel approach that involves subnetworks whose parameters reside in the same space as base learners'. This method aims to maintain performance both on in-domain and out-of-domain tasks, reflected through advanced zero-shot and few-shot scenarios. Unfortunately, the clarity and structure of the manuscript are subpar, making it difficult to follow its methodology and reasoning. Despite interesting empirical results and novel ideas around parameter drift and subnetwork projections, the paper suffers from a lack of detailed motivation, poorly structured experimentation, and ambiguities in the overall research goal which has led to the recommendation of rejection.\n\n**Strengths:**\n- The topic of the paper is viewed as interesting and exciting with a notable focus on the comparison of CLIP and MAML as well as other behavioral insights in meta-learning scenarios.\n- Experimental results are convincingly presented, especially as the tasks progress; the suite of adaptation experiments is extensive and covers a wide range of benchmarks.\n- The paper successfully demonstrates the surpassing of other benchmarks in zero-shot and few-shot learning scenarios.\n- Pseudocode provided in the paper clarifies some of the algorithms used, aiding understanding to some extent.\n\n**Weaknesses:**\n- The writing clarity, structure, and flow of the paper need substantial improvements. Various sections jump without clear transitions, and overall, the paper is hard to follow with an unclear storyline and objectives.\n- The linkage and motivation behind methods and experiments are insufficiently described or justified, making it difficult for readers to grasp the underlying rationales or to replicate the studies.\n- There are several inconsistencies and ambiguities, particularly in the approach to continual learning and the settings of experiments.\n- Related works are inadequately placed in context, with the literature review failing to delineate how the authors’ work aligns with, or differs from, existing studies.\n- Tables and figures are poorly integrated into the text, often lacking accompanying explanations or being awkwardly placed far from the corresponding text references.\n- Empirical backing on some of the claims, such as the composability of subnetwork representations, is missing or not convincingly presented.\n- Technical descriptions and formulations, including those in equations and algorithm pseudocodes, contain errors or are unclear, necessitating thorough revision for accuracy and comprehensibility.\n\n**Questions:**\n- Could the authors clarify the overall structure and intended storyline of the paper? A revised abstract and introduction providing clearer detail on the main contributions and insights might be needed.\n- How do the authors justify the experimental design, particularly the choice of datasets and their usage in training versus validation to avoid potential biases?\n- Can the authors elaborate on the underlying motivations and theoretical foundations of the chosen methodologies, especially how the approach to adapting meta-parameters or base learners in continual learning contexts was formulated?\n- Given the use of meta-learning techniques, how crucial are these to the performance observed, and could similar outcomes be achieved using simpler or alternative methods?\n- Would the authors be able to provide additional empirical evidence or simulations to support claims about the adaptability and composability of subnetwork representations?\n- In regard to technical details: Is a second-order gradient update employed in the meta-learning algorithm? How does the approach affect computational requirements and model complexity?\n\n**Soundness:**\n1 poor\n\n**Presentation:**\n1 poor\n\n**Contribution:**\n2 fair\n\n**Rating:**\n1 strong reject\n\n**Paper Decision:**\n- Decision: Reject\n- Reasons: The reviewers have unanimously highlighted significant concerns regarding the quality of the paper's presentation, unclear justification of goals and methodology, lack of empirical support for some of its claims, and overall readability and structure. These factors significantly detract from the paper's credibility and its fit for a high-quality academic conference. The authors also did not participate in the rebuttal process to address these concerns. The decision to reject is aligned with all reviewers and the academic chair's final recommendation.",
  "content_size": 4526,
  "collected_at": "2026-02-06T14:12:59.169264",
  "compilation_status": "failed",
  "compilation_error": "[WinError 2] 系统找不到指定的文件。",
  "license": "gitlab_repo",
  "license_name": "GitLab Repository",
  "license_url": "https://gitlab.com"
}