{
  "id": "gl_requirementDiagram_00568",
  "source": "gitlab",
  "source_url": "https://github.com/Chrouos/RAG-base-Automatic-Peer-Review/blob/aa1865aded18fdbbdfa055f0cf4f18537243e450/dataset/SEA_ICLR_2024/GPT4_consolidated_reviews_2k/l8je4qJR4K.mmd",
  "source_note": "collected_from_github_additional",
  "github_repo": "Chrouos/RAG-base-Automatic-Peer-Review",
  "diagram_type": "requirementDiagram",
  "code": "**Summary:**\nThe paper introduces a novel two-level latent variable model aiming to enhance domain generalization by distinguishing invariant content factors from variant style factors. This method partitions the latent space into content and style variables, where content variables capture label-related information, and style variables represent domain-specific details. A middle-level latent space derived from high-level latents through label-specific and domain-dependent functions helps achieve this partition. The method embeds this framework into a VAE whose encoder estimates the posterior of the latent factors and employs normalizing flows for transformation between latent levels. An invariant classifier trained on content factors alone theoretically and empirically demonstrates improved domain generalization. This method's core theoretical contribution provides conditions under which content factors are block-identified and style factors linearly identified, crucial for model generalization across varied domains.\n\n**Strengths:**\n- The paper presents an innovative and interesting idea by proposing a novel two-level latent variable model with content/style partition, which contributes to the learning of invariant representations—an important aspect of building generalizable machine learning models.\n- The paper provides a thorough theoretical analysis and identifiability guarantees for the proposed model, introducing conditions for identifiability and isolation of content factors based on exponential family priors and domain variability assumptions.\n- The authors demonstrate strong empirical performance on domain generalization tasks through extensive experiments conducted on both synthetic and real-world datasets, providing convincing results that reinforce their methodology.\n- The methodology integrates VAEs and normalizing flows effectively, representing a promising approach to estimate latent factors for domain generalization that could potentially be applied to other areas beyond image classification.\n- The paper is well-written and well-presented, with clear illustrations that assist in understanding the proposed methods and their implications.\n\n**Weaknesses:**\n- Several reviewers have commented that the paper is hard to read due to confusing notions and a complex formal setup. Simplification of the mathematical notations and clearer explanations, especially in equations such as Eq. (1) and Eq. (7), are suggested to make the paper more accessible.\n- The theoretical analysis, while comprehensive, relies on assumptions such as the exponential family prior and infinite data availability, which may not hold or be practical in real-world scenarios. Discussions on finite sample behavior and the validity of assumptions in more general conditions would strengthen the paper.\n- The generative process and the model’s structure are unclear, notably how the elements such as fy(zc^) and the indexed functions like fe,y,i operate within the framework. Clarification on these parts is crucial.\n- The novelty of the work has been questioned due to similarities with existing methods in feature disentanglement and causality models. Additionally, comparisons with recent and relevant baselines are missing, which could otherwise highlight the advancements over prior works.\n- The application of the proposed method might be limited by the requirement of domain variables which are not always available, and thus its practical applicability could be restricted.\n\n**Questions:**\n- Could the authors elaborate on how they design and differentiate fy and fe,y,i using distinct flow-based architecture to incorporate the information of e, y, i?\n- How does the proposed two-level latent space compare to or connect with hierarchical VAEs, particularly in terms of learning and distinguishing different levels of data features?\n- How exactly does the model's ability to recover the distribution of content factors and separate them from style factors contribute to domain generalization (DG) performance improvement?\n- Could the application of the algorithm to a large-scale dataset like the DomainNet dataset be considered to verify its effectiveness in more challenging scenarios?\n\n**Soundness:**\n3 good\n\n**Presentation:**\n3 good\n\n**Contribution:**\n2 fair\n\n**Rating:**\n5 marginally below the acceptance threshold\n\n**Paper Decision:**\n- Decision: Reject\n- Reasons: The paper, while presenting a promising approach toward domain generalization by distinguishing between content and style latent variables, suffers from a few significant limitations leading to its rejection. The theoretical contributions are criticized for potentially being direct derivations from existing models with only incremental novelty. Additionally, the paper's experiments are seen as limited, not adequately supporting the broad claims of improved domain generalization. The reviewers suggest that more extensive datasets and diverse conditions should be explored to convincingly demonstrate the model's efficacy and generalizability. The clarity of notation and methodology, despite improvements post-rebuttal, was another concern affecting the paper's overall assessment. Therefore, the decision to reject the paper was based on these critical aspects, coupled with marginal empirical improvements reported on constrained dataset choices.",
  "content_size": 5355,
  "collected_at": "2026-02-06T14:12:27.817696",
  "compilation_status": "failed",
  "compilation_error": "[WinError 2] 系统找不到指定的文件。",
  "license": "gitlab_repo",
  "license_name": "GitLab Repository",
  "license_url": "https://gitlab.com"
}