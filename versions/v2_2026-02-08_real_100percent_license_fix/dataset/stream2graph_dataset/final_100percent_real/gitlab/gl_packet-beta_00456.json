{
  "id": "gl_packet-beta_00456",
  "source": "gitlab",
  "source_url": "https://github.com/Chrouos/RAG-base-Automatic-Peer-Review/blob/aa1865aded18fdbbdfa055f0cf4f18537243e450/dataset/ICLR%202024/Reject-standardization_review/dkn9cEOQkU.mmd",
  "source_note": "collected_from_github_additional",
  "github_repo": "Chrouos/RAG-base-Automatic-Peer-Review",
  "diagram_type": "packet-beta",
  "code": "**Summary:**\nThe paper explores real-time reinforcement learning for high-frequency robot control tasks where communication between the action executor and agent can be affected by packet loss and latency. It introduces a novel method using sc-VAE to generate an intermediate representation, which is then used to generate action sequences during execution. This approach aims to address the challenges of fragmentary interaction by allowing the robot to maintain movement even when the latest inference result has not arrived yet. The method was tested in various Mujoco environments and a real-world snake robot control task, showing promising results compared to baselines.\n\n**Strengths:**\n- The paper provides a clear introduction to the background of the real-time RL problem and the research objectives.\n- The proposed method exhibits excellent generality and can work with various reinforcement learning optimization algorithms.\n- The paper offers experimental results on real robots, demonstrating the practicality of the proposed method.\n- The presentation is excellent, with multiple figures helping to clarify the problems.\n- The problem formulation is novel and significant, addressing the issue of fragmentary interaction which is crucial in real-world high-frequency control problems.\n- The method is solid and performs well empirically, as shown in Mujoco experiments which are comparable to baselines in an ideal environment.\n- The ablation study shows the importance of different modules, providing a detailed analysis of the method's components.\n\n**Weaknesses:**\n- The soundness of the paper is limited, particularly the primary claim that \"the action sequences decoded by the close points in the latent space should have a similar influence on the environment\" lacks theoretical explanation.\n- The paper lacks explanations for some critical aspects of the experiments, such as the interaction pattern between the executor and the agent in the real-world experiment.\n- The experimental part may not fully match with the motivation of the paper, particularly in the real-world robotic control experiment which lacks important details.\n- The paper might need to connect more with existing problem formulations, particularly in distinguishing FIMDP from other related problem formulations like POMDP or MDP with reward delays.\n- The robot evaluations are not comprehensive, particularly in real-world environments where fragmentary interaction is critical.\n\n**Questions:**\n- Why is clustering representations of actions that have similar environmental effects better than clustering action sequences with similar values or rewards?\n- In the random FIMDP tasks mentioned, is the number of decision steps fixed within one trial or randomly decided during execution?\n- How does the method perform if trained with longer action sequences but executed with a shorter interval, compared with training with shorter action sequences?\n- Can you provide a more in-depth explanation and analysis on the differences between FIMDP and POMDP, or MDP with reward delays?\n- If we have/learned a world model for the environment, can we do model-based predictions, like model predictive control to solve the FIMDP? How does this compare to learning the multi-step representations?\n- Why does it appear that MARS has a more pronounced advantage over frame-skips in simpler tasks than in more complex tasks?\n- As a robotics-related submission, it is usually good to include demo videos. Could you please share more visualization results of the snake experiment?\n- What is the interaction pattern between the executor and the agent in the real-world experiment?\n\n**Soundness:**\n2 fair\n\n**Presentation:**\n3 good\n\n**Contribution:**\n2 fair\n\n**Rating:**\n5 marginally below the acceptance threshold\n\n**Paper Decision:**\n- Decision: Reject\n- Reasons: The decision to reject is based on several factors. First, the paper's soundness is limited, with the primary claim lacking theoretical explanation and relying heavily on empirical evidence. Second, the experimental results, while promising, do not fully convince of the method's effectiveness in real-world scenarios, particularly in environments where fragmentary interaction is critical. The presentation of the paper is good, but the contribution is considered fair, and the rating is marginally below the acceptance threshold. The decision aligns with the meta-review which emphasizes the need for stronger empirical results and a clearer distinction from existing problem formulations.",
  "content_size": 4530,
  "collected_at": "2026-02-06T14:10:04.932475",
  "compilation_status": "failed",
  "compilation_error": "[WinError 2] 系统找不到指定的文件。",
  "license": "gitlab_repo",
  "license_name": "GitLab Repository",
  "license_url": "https://gitlab.com"
}