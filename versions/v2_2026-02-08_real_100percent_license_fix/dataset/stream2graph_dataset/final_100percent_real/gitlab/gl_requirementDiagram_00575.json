{
  "id": "gl_requirementDiagram_00575",
  "source": "gitlab",
  "source_url": "https://github.com/Chrouos/RAG-base-Automatic-Peer-Review/blob/aa1865aded18fdbbdfa055f0cf4f18537243e450/dataset/NeurIPS%202024/Accept%20(poster)-standardization_review/bPuYxFBHyI.mmd",
  "source_note": "collected_from_github_additional",
  "github_repo": "Chrouos/RAG-base-Automatic-Peer-Review",
  "diagram_type": "requirementDiagram",
  "code": "**Summary:**\nThe paper explores hybrid reinforcement learning (RL) in the context of linear Markov Decision Processes (MDPs), aiming to address the limitations of purely offline and online methods by combining offline data with online exploration. It introduces two algorithms: Reward-Agnostic Pessimistic PAC Exploration-initialized Learning (RAPPEL) and Hybrid Regression for Upper-Confidence Reinforcement Learning (HYRULE). RAPPEL is an online-to-offline approach that enhances the coverage of existing data through reward-agnostic exploration before applying an offline RL method. HYRULE, on the other hand, is an offline-to-online method that leverages offline data to initialize an online algorithm. Both algorithms show improvements over previous methods and meet or exceed the minimax rates in their respective settings. The paper also discusses related works thoroughly and provides theoretical guarantees without relying on the single-policy concentrability assumption.\n\n**Strengths:**\n- The paper is well-organized, with a thorough literature review and clear problem formulation and assumptions.\n- The proposed method significantly improves upon previous hybrid RL methods for linear MDPs, with the online part of Algorithm 2 matching the online minimax rate of linear MDPs.\n- The paper provides no worse than optimal sample complexity for two types of hybrid RL methods, which is non-trivial.\n- The theoretical results are sound, and the proofs appear sound from a quick skim.\n- The paper develops a better theoretical understanding of hybrid RL in linear MDPs, potentially benefiting future works in hybrid RL with functional approximation in this context.\n\n**Weaknesses:**\n- The paper does not introduce new techniques but rather modifies existing algorithms with sharper analysis.\n- The assumption regarding Full Rank Projected Covariates might implicitly impose constraints and is not practical in downstream applications.\n- There is no experimentation or code included with the work, making it difficult to examine whether the proposed algorithms are efficient in practice.\n- Algorithm 2 has a large minimum requirement for the size of the offline dataset.\n- The paper does not clearly summarize or highlight the main technical novelties, and the proofs closely follow existing works.\n\n**Questions:**\n- Could you highlight the main technical techniques/reasons why Algorithm 2 can achieve better regret compared to Tan and Xu, 2024 and Amortila et al., 2024 for the linear MDP case?\n- In the studied setting, if we consider general function approximation or a more general class of MDPs with linear structure, do you envision the current analysis and results can be utilized to improve existing bounds?\n- Could you comment on the optimality of the provided bounds, and whether dependence on the parameters involved can be further improved?\n- What is the range that indicates good concentrability for the concentrability coefficient?\n- In line 135, does d∗ represent the occupancy measure of the optimal policy?\n- It would be helpful if there is an experimental plan to verify the algorithms. A simple toy experiment plan should suffice.\n- Why is |A| in Line 303? If that is not a typo, then it seems to be another weakness of Algorithm 2, since the action space might not be discrete.\n\n**Soundness:**\n3 good\n\n**Presentation:**\n3 good\n\n**Contribution:**\n2 fair\n\n**Rating:**\n6 marginally above the acceptance threshold\n\n**Paper Decision:**\n- Decision: Accept\n- Reasons: The paper presents a technically solid contribution to the field of hybrid RL for linear MDPs, combining online and offline methods to improve upon existing methods. The theoretical contributions are significant and well-supported, with a thorough discussion of related works. The reviewers have highlighted the need for better highlighting of technical contributions and the inclusion of new experiments, which are crucial for the paper's readiness for publication. Despite some concerns about the generalizability of the techniques to other types of function approximations and the lack of experimental validation, the paper's theoretical contributions and methodological soundness justify its acceptance.",
  "content_size": 4194,
  "collected_at": "2026-02-06T14:12:39.459028",
  "compilation_status": "failed",
  "compilation_error": "[WinError 2] 系统找不到指定的文件。",
  "license": "gitlab_repo",
  "license_name": "GitLab Repository",
  "license_url": "https://gitlab.com"
}