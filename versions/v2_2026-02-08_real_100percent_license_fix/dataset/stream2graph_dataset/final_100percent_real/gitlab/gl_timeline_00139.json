{
  "id": "gl_timeline_00139",
  "source": "gitlab",
  "source_url": "https://github.com/Chrouos/RAG-base-Automatic-Peer-Review/blob/aa1865aded18fdbbdfa055f0cf4f18537243e450/dataset/NeurIPS%202024/Accept%20(poster)-standardization_review/PWzB2V2b6R.mmd",
  "source_note": "collected_from_github_additional",
  "github_repo": "Chrouos/RAG-base-Automatic-Peer-Review",
  "diagram_type": "timeline",
  "code": "**Summary:**\nThe paper introduces OV-OAD, a novel zero-shot online action detection system that leverages vision-language models for open-world temporal understanding. The model is designed to recognize actions as soon as they appear in a video stream, without the need for manual annotations. It utilizes a Transformer-based architecture with an object-centered decoder unit and is trained solely on video-text pairs. The system is evaluated on THUMOS'14 and TVSeries benchmarks, demonstrating superior performance over existing zero-shot methods. The authors also provide extensive ablation studies and analyses to validate the model's design choices.\n\n**Strengths:**\n- The paper introduces a significant advancement in action detection by proposing a zero-shot online detection method that does not rely on manual annotations.\n- The proposed model incorporates a novel object-centered decoder unit within the Transformer framework, which is a sophisticated approach to frame aggregation.\n- The paper provides extensive experiments on two benchmarks, demonstrating the effectiveness of the proposed method.\n- Detailed ablation studies and analyses are included, providing comprehensive insights into the architecture design, loss design, and efficiency analysis.\n- The model is impressively fast at inference (292 fps), making it practical for real-world applications such as home security cameras.\n- The paper is well-organized and easy to follow, with clear writing that aids in understanding the complex methodologies employed.\n\n**Weaknesses:**\n- The choice of VLM (Visual Language Model) is questionable as CLIP, used in the model, is an image-based understanding model and lacks the capability to capture temporal context effectively.\n- The comparison of the proposed method with previous work, such as OadTR, seems to use different visual encoders/features, which may not be fair.\n- The model's robustness is questioned, particularly in zero-shot experiments on two datasets, where improvements on the TVSeries dataset are relatively limited compared to THUMOS14.\n- The paper lacks explanations for some operations, such as the operator \"∘\" in Equation 1, which could lead to misunderstandings.\n- There is a discrepancy in the claims about not using frame-level annotations when the Activity Net dataset contains annotations for start and end times for actions.\n- The paper could benefit from comparisons with more recent OAD models such as MAT, MiniROAD, etc., to enhance the timeliness and competitiveness of the model.\n- The Object-Centric Decoder unit is confusingly named and its role in the model is not clearly defined.\n\n**Questions:**\n- Can the authors explain the fairness of the comparison with previous work, particularly in terms of the visual encoders/features used?\n- How does the model achieve ultra-high inference speed with a larger scale of parameters?\n- Could the authors clarify the role of the Object-Centric Decoder unit and its impact on the model's performance?\n- Is there a difference between \"open vocabulary\" and \"zero shot\" action recognition, and how much overlap is there between the textual descriptions in ActivityNet and InternVid with the category label sets of THUMOS'14 and TVSeries?\n- Can the authors provide more details on the assumption of low background information for VLM mentioned in line 36?\n- What precisely is the circle operator in equation (1)?\n- How do the authors know \"when the raw labels are not enough\"?\n- In Table 2, OVOadagger refers to OV-OAD, correct?\n\n**Soundness:**\n3 good\n\n**Presentation:**\n3 good\n\n**Contribution:**\n3 good\n\n**Rating:**\n6 marginally above the acceptance threshold\n\n**Paper Decision:**\n- Decision: Accept\n- Reasons: The paper presents a novel approach to online action detection using pre-trained models, which is technically sound and demonstrates significant improvements over existing methods. The evaluation on THUMOS'14 and TVSeries benchmarks supports the claims of superior performance and inference speed. The reviewers have highlighted some concerns regarding the clarity in certain methodological descriptions and the need for more comparative benchmarks. However, these issues do not significantly detract from the overall strength of the submission. The decision to accept is based on the paper's originality, methodological soundness, and the significant impact it could have on the field of action recognition. The authors are encouraged to address the reviewers' feedback in the camera-ready version.",
  "content_size": 4500,
  "collected_at": "2026-02-06T14:03:33.539964",
  "compilation_status": "failed",
  "compilation_error": "[WinError 2] 系统找不到指定的文件。",
  "license": "gitlab_repo",
  "license_name": "GitLab Repository",
  "license_url": "https://gitlab.com"
}