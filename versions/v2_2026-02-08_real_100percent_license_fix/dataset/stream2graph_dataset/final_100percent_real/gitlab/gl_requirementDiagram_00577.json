{
  "id": "gl_requirementDiagram_00577",
  "source": "gitlab",
  "source_url": "https://github.com/Chrouos/RAG-base-Automatic-Peer-Review/blob/aa1865aded18fdbbdfa055f0cf4f18537243e450/dataset/SEA_ICLR_2024/GPT4_consolidated_reviews_2k/EanCFCwAjM.mmd",
  "source_note": "collected_from_github_additional",
  "github_repo": "Chrouos/RAG-base-Automatic-Peer-Review",
  "diagram_type": "requirementDiagram",
  "code": "**Summary:**\nThis paper introduces a novel method for sparse-view camera pose estimation using a distributed \"ray\" based representation of cameras, specifically through Plucker coordinates. The method involves regressing and diffusing camera rays from sparse RGB images to accurately recover camera parameters. The predictive model shows enhanced performance over existing learning-based and correspondence-based methods in the sparse view setup on the CO3Dv2 dataset. It also demonstrates robustness to out-of-distribution scenes. The primary innovation lies in the use of a ray bundle converted back to standard camera models, which effectively improves accuracy in estimating rotation and pose compared with previous approaches. The proposed method also experiments with a diffusion model to handle uncertainties, enhancing the prediction of camera parameters from images directly.\n\n**Strengths:**\n- The novel method of representing camera pose and intrinsic recovery using a bundle of rays is highly effective, and the ray-based representation suits set-level transformers, showing promising results.\n- The regression-based method clearly outperforms other diffusion-based methods by leveraging over-parameterization for accurate camera geometry solutions.\n- Experimental results on the CO3Dv2 dataset show superior performance in sparse-view settings compared to existing \"leaning\" based and \"correspondence\" based methods. This signifies a commendable advancement in the field.\n- The method also generalizes well to out-of-distribution scenes not originally within the training set.\n- The paper's clarity in writing facilitates understanding, making the innovative approach and its significant impacts in the field of wide-baseline camera estimation easy to grasp.\n\n**Weaknesses:**\n- The experimental setup primarily uses the CO3D dataset, which is somewhat limited and contrived with its predictable, circular camera trajectories around centrally placed objects. This simplification could potentially enhance the proposed method's performance unnaturally.\n- There's a lack of robust testing on varied datasets, particularly those that are scene-centric which could provide a broader validation of the method’s applicability and effectiveness.\n- The paper could benefit from a comprehensive memory requirements assessment, especially considering the potential high GPU memory necessity when processing multiple image features simultaneously.\n- Some visuals and structural presentations within the paper (like Figure 2, 5, and 6 and Table labeling) are confusing and detract from the overall clarity and professionalism of the presentation.\n- More detail on network architecture, training details, and explicit discussion on the novel representation of pose versus traditional representations would strengthen the paper.\n- The potential needs for a subsequent bundle adjustment step to achieve precise camera parameters suggest an incomplete solution for practical application without further processing.\n\n**Questions:**\n- How does the method scale with an increasing number of views, e.g., from 8 to 32 views? What are the memory requirements and computational implications of such scale-ups?\n- For applications that require scaled poses, how can this method be adjusted or used effectively?\n- Can the authors elaborate on how early they stopped the backward diffusion process in their experiments and what criteria were used to decide this timing?\n- Could the authors clarify the application potential for more generic and realistic multi-view imaging environments, particularly those not involving tightly cropped imagery?\n- What is the \"identity\" camera referred to in Eq.5, and are there constraints involved that would affect its general applicability in different scenarios?\n- Why does the performance of the method not consistently improve with the addition of more views, unlike most competing methods?\n- Could you provide rationale or justification for the choice of COLMAP with SuperPoint features and SuperGlue matching used in the experiments?\n\n**Soundness:**\n3 good\n\n**Presentation:**\n2 fair\n\n**Contribution:**\n3 good\n\n**Rating:**\n6 marginally above the acceptance threshold\n\n**Paper Decision:**\n- Decision: Accept\n- Reasons: The paper effectively addresses the challenge of estimating camera pose from sparse-view settings and introduces an innovative ray-bundle representation. This is highlighted by the metareview praising the novelty of the approach and suggesting the model's potential widespread applicability, despite the desire for expanded experimental evaluation. The ability of the ray-based representation to be integrated with modern ML techniques, and its generalization capabilities to environments that are not carefully curated, marks a significant advancement in the field of camera pose estimation. These factors collectively align with the conference criteria emphasizing originality, methodological soundness, and significance of results. However, it is recommended that the authors expand their dataset exploration to further solidify their findings and attend to presentation improvements as suggested.",
  "content_size": 5145,
  "collected_at": "2026-02-06T14:12:42.813392",
  "compilation_status": "failed",
  "compilation_error": "[WinError 2] 系统找不到指定的文件。",
  "license": "gitlab_repo",
  "license_name": "GitLab Repository",
  "license_url": "https://gitlab.com"
}