{
  "id": "gl_block-beta_00335",
  "source": "gitlab",
  "source_url": "https://github.com/Chrouos/RAG-base-Automatic-Peer-Review/blob/aa1865aded18fdbbdfa055f0cf4f18537243e450/dataset/ICLR%202024/Accept%20(poster)-standardization_review/CTlUHIKF71.mmd",
  "source_note": "collected_from_github_additional",
  "github_repo": "Chrouos/RAG-base-Automatic-Peer-Review",
  "diagram_type": "block-beta",
  "code": "**Summary:**\nThe paper introduces Representation-Aligned Preference-based Learning (RAPL), a novel method that leverages human feedback to align visual representations with end-user preferences. RAPL uses a triplet training setup to learn from human preferences, which are then used to optimally transport video embeddings to align with the reward structure for robot policies. The method is tested across various environments, including X-MAGICAL and IsaacGym simulators, demonstrating improved performance over baseline methods in terms of sample efficiency and zero-shot generalization. The approach is evaluated on tasks such as kitchen top cleaning and block rearrangement, showing that RAPL can effectively align visual representations with human preferences and facilitate learning of visual robot rewards.\n\n**Strengths:**\n- The paper presents a novel approach to aligning visual representations to human preferences, which is a significant contribution to the field of robotics.\n- The experiments are well-designed and demonstrate that RAPL outperforms vanilla RLHF in simpler manipulation and block movement tasks, showing promising results.\n- The paper is well-written, making it easy to follow, and includes helpful figures and ablations to aid in understanding the methodology.\n- The results demonstrate zero-shot generalization to new embodiments and sample efficiency, which are crucial benefits of using RAPL.\n- The formalization of the visual representation alignment challenge in robotics as metric learning within the human representation space is a notable contribution.\n\n**Weaknesses:**\n- The experimental setup primarily focuses on simpler tasks, such as avoiding obstacles in the environment, which may not adequately challenge the methodology.\n- The paper lacks a comparison to more complex baselines, such as Zhang et al. (2020), which could provide a more robust evaluation of the method.\n- There is a lack of discussion on how the visual attention map is computed, which is crucial for understanding the method's effectiveness.\n- The paper does not consider multi-modal preferences, which could significantly impact the generalizability and applicability of the method in real-world scenarios.\n- The use of a simulated human model for data generation raises questions about the method's effectiveness in real-world applications.\n\n**Questions:**\n- How is the proposed approach better than GT for \"Avoiding\"?\n- Can the authors clarify how the visual attention map is computed and provide more details on its computation?\n- How many trials are included in the experiments where error bars are shown?\n- Could the authors elaborate on their method of utilizing privileged access to state information for dataset generation?\n- Have the authors tried scaling training for RLHF to see how long it takes to reach RAPL/GT performance?\n- How does the proposed approach perform in more complex tasks, such as block stacking with constraints where a user prefers stacking based on size?\n- Could the authors compare the performance of RAPL to a simple contrastive pretrained visual representation with the same optimal transport logic?\n\n**Soundness:**\n2 fair\n\n**Presentation:**\n3 good\n\n**Contribution:**\n2 fair\n\n**Rating:**\n5 marginally below the acceptance threshold\n\n**Paper Decision:**\n- Decision: Accept\n- Reasons: The paper presents a novel approach to aligning visual representations to human preferences, which is a significant contribution to the field of robotics. The experimental results demonstrate that RAPL outperforms baseline methods in simpler manipulation and block movement tasks, showing promising results. The methodology is well-explained, and the paper is well-written, making it easy to follow. The method's ability to generalize to new embodiments and its sample efficiency are notable strengths. However, the experimental setup primarily focuses on simpler tasks, and the method's effectiveness in more complex scenarios remains unclear. The paper could benefit from a more rigorous comparison to more complex baselines and a discussion on how the visual attention map is computed. Despite these limitations, the paper's contributions and the experimental results justify its acceptance.",
  "content_size": 4225,
  "collected_at": "2026-02-06T14:07:28.396292",
  "compilation_status": "failed",
  "compilation_error": "[WinError 2] 系统找不到指定的文件。",
  "license": "gitlab_repo",
  "license_name": "GitLab Repository",
  "license_url": "https://gitlab.com"
}