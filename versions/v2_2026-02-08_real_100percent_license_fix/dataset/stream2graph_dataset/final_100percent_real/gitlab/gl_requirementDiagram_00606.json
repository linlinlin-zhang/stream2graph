{
  "id": "gl_requirementDiagram_00606",
  "source": "gitlab",
  "source_url": "https://github.com/Chrouos/RAG-base-Automatic-Peer-Review/blob/aa1865aded18fdbbdfa055f0cf4f18537243e450/dataset/SEA_ICLR_2024/GPT4_consolidated_reviews_2k/bDZCBjVgKW.mmd",
  "source_note": "collected_from_github_additional",
  "github_repo": "Chrouos/RAG-base-Automatic-Peer-Review",
  "diagram_type": "requirementDiagram",
  "code": "**Summary:**\nThe paper presents a study on enhancing the quality of Neural Radiance Fields (NeRF) rendered images through two innovative tools: a visibility prediction network (VPN) and a visibility scoring function. These tools aim to predict and utilize visibility information to select additional views for retraining NeRF and to reduce rendering artifacts by discarding unreliable near-range points. The VPN leverages visibility data from density values to down-weight obscure 3D points during volumetric rendering or to choose extra training views. The scoring function assesses the reliability of rendered points for artifact removal and image quality improvement. However, the novelty of the approach is questioned since similar ideas have been explored in previous works. Furthermore, the feasibility of implementing these tools is scrutinized due to the extensive computational demand and the absence of a common benchmark dataset, which limits the evaluation's thoroughness.\n\n**Strengths:**\n- The paper introduces a novel method to distill visibility information from density values to a lightweight network, effectively incorporating it to improve the quality of NeRF.\n- The paper is well written, providing ease of readability and thorough up-to-date citations of relevant recent advances.\n- Experimental results strongly verify the improvement in rendering quality, demonstrating the method's effectiveness on the Nerfacto variant and potentially other ray-casting based NeRF variants.\n- The paper is well-motivated, addressing an important problem of judging the confidence of NeRFs particularly in scenarios lacking camera pose information.\n\n**Weaknesses:**\n- The paper lacks discussion of prior work that investigates integrating visibility information into neural implicit representations, such as \"Neural Rays for Occlusion-aware Image-based Rendering\" which was presented at CVPR 2022.\n- There is an over-reliance on a heavy computational grid alongside the neural network, which brings feasibility concerns for the method as a post-processing step due to the large grid size needed for datasets with hundreds of views.\n- Experiments are only conducted on a dataset collected by the authors and need to be expanded to well-known NeRF datasets to better support the proposed method and generalize findings.\n- The method's requirement for concurrent training with NeRF and not merely as a post-training process is not straightforward and adds complexity.\n- The introduction and various figures lack clarity and important citations or explanations, potentially hindering comprehensibility and undermining the paper's impact and novelty.\n- Details about the dataset, named ObjectScan, whether it will be released, and its contribution to the field are not clearly stated, leading to confusion about its use and availability.\n\n**Questions:**\n- Can you clarify if the density visibility prediction model tends to produce holes in rendering results as seen in your experiments? How does the model generally handle scenarios with varying visibility?\n- Please clarify the source of the ObjectScan dataset and its release conditions. Is this a proprietary dataset created by the authors or an existing benchmark that has not been cited properly?\n- In instances where a grid is used for training the visibility prediction network, is it also utilized during inference? If not, how does the model maintain accuracy without the multiplicative data from the grid?\n- Could you elaborate on how the visibility prediction network influences the training behavior of NeRF when optimized with different loss functions simultaneously?\n- Concerning the method for skipping low-visible near-range points to reduce rendering artifacts: how are visibility and depth thresholds determined across varied scenes to ensure consistency?\n- Additional question about the computational heavy lifting: considering the heavy computational grid, could you detail more about the training procedure, the execution time, and its practical feasibility as a post-processing method?\n\n**Soundness:**\n3 good\n\n**Presentation:**\n3 good\n\n**Contribution:**\n2 fair\n\n**Rating:**\n3 reject, not good enough\n\n**Paper Decision:**\n- Decision: Reject\n- Reasons: The paper introduces conceptually interesting approaches to enhance NeRF-rendered images using visibility-based tools. However, its novelty is limited due to insufficient citation of existing similar works. The method's practical implications are under scrutiny due to the extensive computational requirements described, which could hamper its feasibility as a post-processing step. There is also a notable lack of a thorough evaluation using well-established benchmarks, and the self-collected dataset used is not disclosed for external validation. These gaps critically impair the paper's impact and verifiability. The reviewers recommend rejection at this stage, suggesting a comprehensive enhancement in novelty, methodological depth, and experimental validation for future submissions.",
  "content_size": 5016,
  "collected_at": "2026-02-06T14:13:19.532533",
  "compilation_status": "failed",
  "compilation_error": "[WinError 2] 系统找不到指定的文件。",
  "license": "gitlab_repo",
  "license_name": "GitLab Repository",
  "license_url": "https://gitlab.com"
}