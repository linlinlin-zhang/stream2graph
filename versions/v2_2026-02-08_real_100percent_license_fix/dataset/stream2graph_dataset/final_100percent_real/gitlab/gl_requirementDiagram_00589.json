{
  "id": "gl_requirementDiagram_00589",
  "source": "gitlab",
  "source_url": "https://github.com/Chrouos/RAG-base-Automatic-Peer-Review/blob/aa1865aded18fdbbdfa055f0cf4f18537243e450/dataset/ICLR%202024/Accept%20(poster)-standardization_review/c0MyyXyGfn.mmd",
  "source_note": "collected_from_github_additional",
  "github_repo": "Chrouos/RAG-base-Automatic-Peer-Review",
  "diagram_type": "requirementDiagram",
  "code": "**Summary:**\nThe paper introduces a novel method called prioritized soft Q-decomposition (PSQD) to address lexicographic multi-objective reinforcement learning (MORL) problems. This method leverages the value function of previously learned subtasks to constrain the action space of subsequent subtasks, ensuring that higher-priority tasks are optimized while lower-priority tasks are adapted to maintain the highest priority constraint. The approach is tested in various scenarios, demonstrating its effectiveness in optimizing policies under priority constraints. The methodology involves creating zero-shot Q-functions and strategies for priority-constrained tasks by transforming individual subtasks within the indifference space of actions for higher-priority tasks.\n\n**Strengths:**\n- The paper addresses two significant problems in Reinforcement Learning for control tasks: providing an effective means of composing overall behavior from learned subtasks without significant need for new data collection and re-learning, and enforcing strict priority constraints during composition and subsequent learning to optimize from the initial one-shot policy.\n- The methodology and corresponding learning algorithm to form RL strategies by composing subtask functions are novel and sound, as evidenced by mathematical formulations and extensive study on previous works.\n- The paper is well-organized, easy to read, and provides a dense appendix for detailed explanations.\n- The proposed method manages to be effective in tackling the problem, and the mathematical formulations support the soundness of the algorithm.\n\n**Weaknesses:**\n- The paper lacks a discussion of the full range of situations, particularly in the presentation of the underpinnings of the framework. Specifically, there is a complete absence of discussion on cases where subtasks might not be compatible, which could lead to an empty indifference space of actions for higher priority tasks.\n- The presentation needs improvement, particularly in explaining the flow of the algorithm and providing pseudocode for clarity.\n- The paper is sensitive to the parameter ε, and the manual selection of this parameter is non-trivial.\n- There is a lack of comparative analysis with existing lexicographic MORL algorithms, which could enhance the paper's contribution.\n- The experiments deal only with two subtasks, and it would be beneficial to see how the method performs in more complex scenarios involving multiple subtasks.\n\n**Questions:**\n- How does the proposed approach deal with incompatible subtasks? Does it simply eliminate all tasks with empty indifference action sets for higher priority tasks and then operate in the same way as presented in the paper?\n- How sensitive is the approach to the specific choice of ε thresholds? Is there a way that an ablation could be performed that would investigate the sensitivity of the top priority task's threshold in the navigation experiments?\n- In number 2 in Weakness, if one of the candidates is PSQD, do the authors think that the other one is also a valid algorithm?\n- Could the authors clarify the relationship between equations (1) and (7) and provide more theoretical insights to enhance the paper's rigor?\n- Is it possible to find a constant εi that precisely represents the task requirements, or should εi be state-dependent?\n\n**Soundness:**\n3 good\n\n**Presentation:**\n2 fair\n\n**Contribution:**\n3 good\n\n**Rating:**\n6 marginally above the acceptance threshold\n\n**Paper Decision:**\n- Decision: Accept\n- Reasons: The paper presents a novel method that effectively addresses the challenges of lexicographic MORL problems by leveraging the value function of previously learned subtasks to constrain the action space of subsequent subtasks. The methodology is sound, and the experimental results substantiate its efficacy in optimizing policies under priority constraints. While there are some weaknesses in the presentation and a lack of discussion on certain aspects of the framework, these do not significantly detract from the overall contribution of the paper. The decision to accept is based on the originality of the approach, methodological soundness, and the significance of the results, which are supported by empirical evidence.",
  "content_size": 4251,
  "collected_at": "2026-02-06T14:13:00.553460",
  "compilation_status": "failed",
  "compilation_error": "[WinError 2] 系统找不到指定的文件。",
  "license": "gitlab_repo",
  "license_name": "GitLab Repository",
  "license_url": "https://gitlab.com"
}