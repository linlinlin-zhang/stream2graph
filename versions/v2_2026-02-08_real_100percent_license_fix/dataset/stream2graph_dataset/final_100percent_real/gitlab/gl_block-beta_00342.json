{
  "id": "gl_block-beta_00342",
  "source": "gitlab",
  "source_url": "https://github.com/Chrouos/RAG-base-Automatic-Peer-Review/blob/aa1865aded18fdbbdfa055f0cf4f18537243e450/dataset/NeurIPS%202024/Accept%20(poster)-standardization_review/7txPaUpUnc.mmd",
  "source_note": "collected_from_github_additional",
  "github_repo": "Chrouos/RAG-base-Automatic-Peer-Review",
  "diagram_type": "block-beta",
  "code": "**Summary:**\nThe paper introduces a novel approach to training sparse auto-encoders (SAEs) by replacing the traditional reconstruction loss with a KL-divergence loss, aiming to improve the interpretability of large language models (LLMs). This method encourages SAEs to learn features that are causally relevant to the model's output by optimizing the KL divergence between the model's output distribution and the distribution when activations are replaced by their reconstructions. The authors also incorporate a sparsity penalty and additional loss terms to encourage reconstructions that lead to downstream layer activations similar to the original downstream layer activations. The proposed method is evaluated against various baselines and shows improvements in terms of sparsity and loss recovered when using reconstructions.\n\n**Strengths:**\n- The paper introduces a new training method for Sparse Autoencoders (SAEs) with an end-to-end loss that allows the use of fewer features from the original model, which is a Pareto improvement over existing techniques on L0 vs. CE loss.\n- The investigation of the geometry of the different SAE features is interesting, particularly the finding that e2e + ds finds similar features to local and is reproducible over different seeds, and e2e has potentially less feature splitting.\n- The methodology is careful, and the paper considers alternative hypotheses or potential pitfalls in the analyses, making it a robust contribution to the field.\n- The writing is mostly clear and easy to follow, and the related work is sufficient.\n- The paper discusses the limitations of its approach, such as robustness to random seeds, reconstruction loss, and training time.\n\n**Weaknesses:**\n- The contribution is not very groundbreaking, and there are no new findings regarding interpretability.\n- The modification is simple, and it is unclear whether additional variants could work, such as local SAEs with MSE end-to-end loss.\n- The numeric results are very incremental, and the interpretability is not well defined. The claim that fewer features mean more interpretability should be proved by empirical evidence.\n- The paper lacks a clear conclusion or take-home message, and the comparison to local SAEs is sometimes unclear.\n- The evaluation metrics are somewhat indirect, and the paper does not adequately address the Goodhart's law problem.\n- The paper could benefit from additional metrics for downstream SAE quality and a more fine-grained picture of how individual features change between the vanilla SAE and the e2e ones.\n\n**Questions:**\n- Can you explain the diagram in Figure 1 why some blocks are of different shapes?\n- It looks like the Sec. 3.4 result is trivial. If we don’t encourage the SAE to reconstruct after each activation, that will be the result. What is the number of parameters you train in SAE in total for all layers?\n- Why doesn't the original SAE learn the \"left block\" of features that SAE e2e + ds does? It would be great to see if these have a small or negative impact on the variance explained.\n- Did the authors try training an SAE with KL divergence + the local reconstruction error (i.e., MSE of layer X reconstruction for an SAE trained on layer X)?\n- Can you quantify how much farther the <e2e ds to local> difference is vs. the inter-seed distance? It is hard to know how to interpret Appendix G without this context, as inter-seed SAEs have different features learned too.\n- The ~0.15 CE loss e2e+downstream SAE looks closer to the other chosen SAEs than the 0.125 CE loss one. Does it change things to run with the ~0.15 one?\n\n**Soundness:**\n3 good\n\n**Presentation:**\n3 good\n\n**Contribution:**\n3 good\n\n**Rating:**\n6 marginally above the acceptance threshold\n\n**Paper Decision:**\n- Decision: Accept\n- Reasons: The paper presents a well-justified method for training sparse auto-encoders that encourages them to learn features relevant to the model's output. The method is supported by empirical results that demonstrate improved performance and a geometric study of the learnt features. The reviewers found the reasoning of the paper convincing, and the limitations well-covered. The paper's transparency on the limitations of the approach and the experiments, along with the steps forward for SAE interpretation of LLMs, make it a valuable contribution to the field. Despite some concerns about the interpretability of the evaluation metrics and the incremental improvements, the overall consensus leans towards acceptance.",
  "content_size": 4500,
  "collected_at": "2026-02-06T14:07:41.223168",
  "compilation_status": "failed",
  "compilation_error": "[WinError 2] 系统找不到指定的文件。",
  "license": "gitlab_repo",
  "license_name": "GitLab Repository",
  "license_url": "https://gitlab.com"
}