{
  "id": "gh_architecture_01673",
  "source": "github",
  "source_url": "https://github.com/SolaceHarmony/Thea-Code/blob/dac98bfba1d47786ebee1b4b03242340e17abbd2/flowchart%20TB.mmd",
  "github_repo": "SolaceHarmony/Thea-Code",
  "github_file_path": "flowchart TB.mmd",
  "diagram_type": "architecture",
  "code": "flowchart TB\n    classDef inputClass fill:#FFD700,stroke:#FF8C00,stroke-width:2px,color:#000\n    classDef embeddingClass fill:#98FB98,stroke:#006400,stroke-width:2px,color:#000\n    classDef rwkvClass fill:#E6E6FA,stroke:#483D8B,stroke-width:2px,color:#000\n    classDef ffnClass fill:#FFA07A,stroke:#8B0000,stroke-width:2px,color:#000\n    classDef normClass fill:#D8BFD8,stroke:#9400D3,stroke-width:2px,color:#000\n    classDef outputClass fill:#F08080,stroke:#CD5C5C,stroke-width:2px,color:#000\n    \n    %% Input Processing\n    subgraph InputProcessing[\"Input Processing Layer\"]\n        direction TB\n        Input[/\"Input Tokens\"/]:::inputClass\n        TokenEmbed[\"Token Embeddings<br/>(d_model)\"]:::embeddingClass\n        TimeEmbed[\"Time Embeddings<br/>(d_model)\"]:::embeddingClass\n        \n        Input --> TokenEmbed\n        TokenEmbed --> InputEmbed\n        TimeEmbed --> InputEmbed\n        \n        InputEmbed[\"Input Embeddings<br/>(batch_size × seq_len × d_model)\"]:::embeddingClass\n    end\n    \n    %% Annotation for Input Processing\n    InputAnnotation[\"<b>Input Processing</b><br/>- Tokens converted to embeddings<br/>- Time embeddings instead of positional<br/>- Dimensions: batch_size × seq_len × d_model\"]\n    InputProcessing -.-> InputAnnotation\n    \n    %% First RWKV Layer\n    subgraph Layer1[\"RWKV Layer 1\"]\n        direction TB\n        \n        %% Time-mixing Block (Alternative to Attention)\n        subgraph TMB1[\"Time Mixing Block\"]\n            direction TB\n            \n            %% Time Mixing Components\n            TMNorm1[\"Layer Normalization\"]:::normClass\n            TMLinear1[\"Linear Projection\"]:::rwkvClass\n            TMWKV1[\"WKV Mechanism<br/>(Receptance & Decay)\"]:::rwkvClass\n            TMOutput1[\"Time-Mixed Output\"]:::rwkvClass\n            \n            %% Time Mixing Flow\n            TMNorm1 --> TMLinear1\n            TMLinear1 --> TMWKV1\n            TMWKV1 --> TMOutput1\n        end\n        \n        %% Channel Mixing Block (FFN alternative)\n        subgraph CMB1[\"Channel Mixing Block\"]\n            direction TB\n            CMNorm1[\"Layer Normalization\"]:::normClass\n            CMRLinear1[\"Receptance Gate\"]:::rwkvClass\n            CMKLinear1[\"Key Projection\"]:::rwkvClass\n            CMVLinear1[\"Value Projection\"]:::rwkvClass\n            CMOutput1[\"Channel-Mixed Output\"]:::rwkvClass\n            \n            %% Channel Mixing Flow\n            CMNorm1 --> CMRLinear1 & CMKLinear1 & CMVLinear1\n            CMRLinear1 & CMKLinear1 & CMVLinear1 --> CMOutput1\n        end\n        \n        %% Residual Connections\n        Add1((\"+\"))\n        Add2((\"+\"))\n    end\n    \n    %% Annotation for Time Mixing\n    TMAnnotation[\"<b>Time Mixing Block</b><br/>- Linear attention alternative<br/>- Recurrent formulation with tokens<br/>- WKV mechanism with exponential decay<br/>- O(1) complexity per token vs O(n²)\"]\n    TMB1 -.-> TMAnnotation\n    \n    %% Annotation for Channel Mixing\n    CMAnnotation[\"<b>Channel Mixing Block</b><br/>- FFN replacement with gating<br/>- Mixes information across channels<br/>- Uses receptance gating mechanism<br/>- Similar to SwiGLU in transformers\"]\n    CMB1 -.-> CMAnnotation\n    \n    %% Second RWKV Layer\n    subgraph Layer2[\"RWKV Layer 2\"]\n        direction TB\n        \n        %% Time Mixing Block\n        subgraph TMB2[\"Time Mixing Block\"]\n            direction TB\n            TMNorm2[\"Layer Normalization\"]:::normClass\n            TMLinear2[\"Linear Projection\"]:::rwkvClass\n            TMWKV2[\"WKV Mechanism<br/>(Receptance & Decay)\"]:::rwkvClass\n            TMOutput2[\"Time-Mixed Output\"]:::rwkvClass\n            \n            TMNorm2 --> TMLinear2\n            TMLinear2 --> TMWKV2\n            TMWKV2 --> TMOutput2\n        end\n        \n        %% Channel Mixing Block\n        subgraph CMB2[\"Channel Mixing Block\"]\n            direction TB\n            CMNorm2[\"Layer Normalization\"]:::normClass\n            CMRLinear2[\"Receptance Gate\"]:::rwkvClass\n            CMKLinear2[\"Key Projection\"]:::rwkvClass\n            CMVLinear2[\"Value Projection\"]:::rwkvClass\n            CMOutput2[\"Channel-Mixed Output\"]:::rwkvClass\n            \n            CMNorm2 --> CMRLinear2 & CMKLinear2 & CMVLinear2\n            CMRLinear2 & CMKLinear2 & CMVLinear2 --> CMOutput2\n        end\n        \n        %% Residual Connections\n        Add3((\"+\"))\n        Add4((\"+\"))\n    end\n    \n    %% Nth RWKV Layer (Abbreviated)\n    subgraph LayerN[\"RWKV Layer N\"]\n        direction TB\n        \n        %% Abbreviated components\n        TMBN[\"Time<br/>Mixing<br/>Block\"]:::rwkvClass\n        CMBN[\"Channel<br/>Mixing<br/>Block\"]:::rwkvClass\n        \n        %% Residual Connections\n        AddN1((\"+\"))\n        AddN2((\"+\"))\n        \n        %% Connections\n        TMBN --> AddN1\n        AddN1 --> CMBN\n        CMBN --> AddN2\n    end\n    \n    %% Output Processing\n    subgraph OutputProcessing[\"Output Processing\"]\n        direction TB\n        FinalLayerNorm[\"Final Layer Normalization\"]:::normClass\n        OutputProjection[\"Output Projection<br/>(token prediction)\"]:::outputClass\n        FinalOutput[/\"Next Token Output\"/]:::outputClass\n        \n        FinalLayerNorm --> OutputProjection\n        OutputProjection --> FinalOutput\n    end\n    \n    %% Annotation for RWKV Formula\n    RWKVFormula[\"<b>RWKV Formula</b><br/>- WKV(t) = ∑(k=1 to t) W_k × V_k × exp(-decay × (t-k))<br/>- Output = sigmoid(R) ⊙ WKV / (∑exp(-decay))<br/>- Linear time & constant memory complexity<br/>- Parallelizable in training, recursive in inference\"]\n    Layer2 -.-> RWKVFormula\n    \n    %% Annotation for Architecture Comparison\n    ComparisonAnnotation[\"<b>RWKV vs Transformer</b><br/>- Linear O(n) vs Quadratic O(n²) complexity<br/>- State-based vs All-to-all attention<br/>- Can run as CNN or RNN<br/>- Channel mixing vs FFN<br/>- Comparable quality at higher efficiency\"]\n    LayerN -.-> ComparisonAnnotation\n    \n    %% Main Flow Connections\n    InputProcessing --> Layer1\n    InputEmbed --> Add1\n    \n    %% Layer 1 Connections\n    Add1 --> TMNorm1\n    TMOutput1 --> Add1\n    Add1 --> Add2\n    Add2 --> CMNorm1\n    CMOutput1 --> Add2\n    \n    %% Between Layer 1 and Layer 2\n    Add2 --> Layer2\n    Add2 --> Add3\n    \n    %% Layer 2 Connections\n    Add3 --> TMNorm2\n    TMOutput2 --> Add3\n    Add3 --> Add4\n    Add4 --> CMNorm2\n    CMOutput2 --> Add4\n    \n    %% Between Layer 2 and Layer N\n    Add4 --> LayerN\n    Add4 --> AddN1\n    \n    %% Layer N Connections\n    AddN2 --> OutputProcessing\n    AddN2 --> FinalLayerNorm\n    \n    %% Dual Mode Annotation\n    DualModeAnnotation[\"<b>RWKV Dual Mode</b><br/>- Training: Parallel mode (CNN-like)<br/>- Inference: Recurrent mode (RNN-like)<br/>- Same parameters, two execution strategies<br/>- Enables both training efficiency and<br/>  inference performance\"]\n    OutputProcessing -.-> DualModeAnnotation",
  "content_size": 6771,
  "collected_at": "2026-02-06T02:01:57.630696",
  "license": "apache-2.0",
  "license_name": "Apache License 2.0",
  "compilation_status": "success",
  "compilation_error": "[WinError 2] 系统找不到指定的文件。",
  "license_url": "https://api.github.com/licenses/apache-2.0",
  "repo_stars": 0,
  "repo_forks": 0
}